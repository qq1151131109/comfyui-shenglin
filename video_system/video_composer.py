"""
è§†é¢‘åˆæˆå™¨èŠ‚ç‚¹ - å®Œæ•´ç‰ˆ
åŸºäºåŸCozeå·¥ä½œæµè®¾è®¡ï¼Œé›†æˆåŠ¨ç”»æ•ˆæœå’Œè½¬åœºå¤„ç†
å°†éŸ³é¢‘åˆ—è¡¨å’Œå›¾ç‰‡åˆ—è¡¨åˆæˆä¸ºå…·æœ‰ä¸“ä¸šæ•ˆæœçš„MP4è§†é¢‘
"""

import os
import tempfile
import subprocess
import shutil
import torch
import torchaudio
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import cv2
import folder_paths
from typing import List, Dict, Any, Tuple, Generator
import json
import math

class VideoComposer:
    """
    è§†é¢‘åˆæˆå™¨ - åŸºç¡€éŸ³è§†é¢‘åŒæ­¥åˆæˆ

    å°†éŸ³é¢‘åˆ—è¡¨å’Œå›¾ç‰‡batchåˆæˆä¸ºæ—¶é—´åŒæ­¥çš„è§†é¢‘
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_list": ("*", {"tooltip": "éŸ³é¢‘åˆ—è¡¨ï¼Œæ¯ä¸ªéŸ³é¢‘å¯¹åº”ä¸€ä¸ªåœºæ™¯"}),
                "images": ("IMAGE", {"tooltip": "å›¾ç‰‡batchï¼Œæ¯å¼ å›¾ç‰‡å¯¹åº”ä¸€ä¸ªåœºæ™¯"}),
                "fps": ("INT", {
                    "default": 30,
                    "min": 15,
                    "max": 60,
                    "step": 1,
                    "tooltip": "è§†é¢‘å¸§ç‡"
                }),
                "width": ("INT", {
                    "default": 720,
                    "min": 480,
                    "max": 1920,
                    "step": 8,
                    "tooltip": "è§†é¢‘å®½åº¦"
                }),
                "height": ("INT", {
                    "default": 1280,
                    "min": 720,
                    "max": 2560,
                    "step": 8,
                    "tooltip": "è§†é¢‘é«˜åº¦"
                })
            },
            "optional": {
                "output_format": (["mp4", "avi", "mov"], {
                    "default": "mp4",
                    "tooltip": "è¾“å‡ºè§†é¢‘æ ¼å¼"
                }),
                "quality": (["high", "medium", "low"], {
                    "default": "medium",
                    "tooltip": "è§†é¢‘è´¨é‡"
                }),
                "animation_type": (["coze_zoom", "fade", "slide", "none"], {
                    "default": "coze_zoom",
                    "tooltip": "åŠ¨ç”»æ•ˆæœç±»å‹"
                }),
                "transition_duration": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "è½¬åœºæ—¶é•¿ï¼ˆç§’ï¼‰"
                })
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("video_path", "info")
    FUNCTION = "compose_video"
    CATEGORY = "ğŸ¬ Video"
    DESCRIPTION = "å°†éŸ³é¢‘åˆ—è¡¨å’Œå›¾ç‰‡åˆæˆä¸ºåŒæ­¥è§†é¢‘"

    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()

    def compose_video(self, audio_list, images, fps=30, width=720, height=1280,
                     output_format="mp4", quality="medium", animation_type="coze_zoom",
                     transition_duration=0.5):
        """
        åˆæˆè§†é¢‘çš„ä¸»å‡½æ•°
        """
        try:
            # æ£€æŸ¥è¾“å…¥
            if not isinstance(audio_list, list):
                raise ValueError("audio_listå¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹")

            if len(audio_list) == 0:
                raise ValueError("éŸ³é¢‘åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

            if images.shape[0] != len(audio_list):
                print(f"âš ï¸ è­¦å‘Šï¼šå›¾ç‰‡æ•°é‡({images.shape[0]}) ä¸éŸ³é¢‘æ•°é‡({len(audio_list)}) ä¸åŒ¹é…")
                # è°ƒæ•´åˆ°æœ€å°é•¿åº¦
                min_count = min(images.shape[0], len(audio_list))
                images = images[:min_count]
                audio_list = audio_list[:min_count]

            print(f"ğŸ¬ å¼€å§‹è§†é¢‘åˆæˆï¼š{len(audio_list)} ä¸ªåœºæ™¯ï¼Œåˆ†è¾¨ç‡ {width}x{height}")

            # 1. åˆ†æéŸ³é¢‘æ—¶é•¿
            audio_durations = []
            total_duration = 0

            for i, audio_dict in enumerate(audio_list):
                if isinstance(audio_dict, dict) and "waveform" in audio_dict:
                    waveform = audio_dict["waveform"]
                    sample_rate = audio_dict["sample_rate"]

                    # è®¡ç®—æ—¶é•¿ï¼ˆç§’ï¼‰
                    if len(waveform.shape) == 3:
                        waveform = waveform[0]  # ç§»é™¤batchç»´åº¦

                    duration = waveform.shape[1] / sample_rate
                    audio_durations.append(duration)
                    total_duration += duration
                    print(f"ğŸµ åœºæ™¯ {i+1} éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")
                else:
                    raise ValueError(f"éŸ³é¢‘ {i} æ ¼å¼ä¸æ­£ç¡®ï¼Œéœ€è¦åŒ…å«waveformå’Œsample_rate")

            # 2. æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘
            print("ğŸ”Š æ‹¼æ¥éŸ³é¢‘...")
            combined_audio_path = self._combine_audio(audio_list)

            # 3. è®¡ç®—æ€»å¸§æ•°
            total_frames = sum(int(duration * fps) for duration in audio_durations)
            transition_frames_total = int(transition_duration * fps) * (len(images) - 1) if transition_duration > 0 else 0
            total_frames += transition_frames_total

            print(f"ğŸ“Š é¢„è®¡ç”Ÿæˆ {total_frames} å¸§ (å†…å­˜ä¼˜åŒ–æ¨¡å¼)")

            # 4. åˆæˆæœ€ç»ˆè§†é¢‘ï¼ˆä½¿ç”¨æµå¼å¤„ç†ï¼‰
            print("ğŸ¬ å¼€å§‹æµå¼è§†é¢‘åˆæˆ...")
            output_filename = f"story_video_{self._get_timestamp()}.{output_format}"
            video_path = os.path.join(self.output_dir, output_filename)

            # åˆ›å»ºå¸§ç”Ÿæˆå™¨
            frame_generator = self._create_animated_frame_generator(
                images, audio_durations, fps, width, height,
                animation_type, transition_duration
            )

            self._merge_video_audio_streaming(
                frame_generator, combined_audio_path, video_path, fps, quality, total_frames
            )

            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(combined_audio_path)
            except:
                pass

            info = (f"è§†é¢‘åˆæˆå®Œæˆ\\n"
                   f"åœºæ™¯æ•°: {len(audio_list)}\\n"
                   f"æ€»æ—¶é•¿: {total_duration:.2f}ç§’\\n"
                   f"åˆ†è¾¨ç‡: {width}x{height}\\n"
                   f"å¸§ç‡: {fps}fps\\n"
                   f"è¾“å‡º: {output_filename}")

            print(f"âœ… è§†é¢‘åˆæˆå®Œæˆ: {video_path}")
            return (video_path, info)

        except Exception as e:
            error_msg = f"è§†é¢‘åˆæˆå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return ("", error_msg)

    def _combine_audio(self, audio_list):
        """æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘"""
        waveforms = []
        sample_rate = None

        for audio_dict in audio_list:
            waveform = audio_dict["waveform"]
            if len(waveform.shape) == 3:
                waveform = waveform[0]  # ç§»é™¤batchç»´åº¦

            waveforms.append(waveform)
            sample_rate = audio_dict["sample_rate"]

        # æ‹¼æ¥éŸ³é¢‘
        combined_waveform = torch.cat(waveforms, dim=1)

        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        torchaudio.save(temp_file.name, combined_waveform, sample_rate)
        temp_file.close()

        return temp_file.name

    def _create_animated_video_frames(self, images, durations, fps, width, height,
                                    animation_type, transition_duration):
        """
        åˆ›å»ºå¸¦åŠ¨ç”»æ•ˆæœçš„è§†é¢‘å¸§ - åŸºäºåŸCozeå·¥ä½œæµè®¾è®¡

        æ”¯æŒçš„åŠ¨ç”»ç±»å‹ï¼š
        - coze_zoom: åŸCozeå·¥ä½œæµçš„ç¼©æ”¾åŠ¨ç”»ï¼ˆå¥‡å¶äº¤æ›¿æ–¹å‘ï¼‰
        - fade: æ·¡å…¥æ·¡å‡ºæ•ˆæœ
        - slide: æ»‘åŠ¨æ•ˆæœ
        - none: æ— åŠ¨ç”»
        """
        all_frames = []
        transition_frames = int(transition_duration * fps) if transition_duration > 0 else 0

        for i, (image_tensor, duration) in enumerate(zip(images, durations)):
            # è½¬æ¢tensoråˆ°PILå›¾ç‰‡
            image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(image_np)

            # è°ƒæ•´å›¾ç‰‡å°ºå¯¸åˆ°ç•¥å¤§äºç›®æ ‡ï¼Œç”¨äºç¼©æ”¾åŠ¨ç”»
            if animation_type == "coze_zoom":
                # Cozeé£æ ¼ï¼šå›¾ç‰‡ç¨å¤§ä¸€äº›ï¼Œç”¨äºç¼©æ”¾
                scale_factor = 1.3
                scaled_width = int(width * scale_factor)
                scaled_height = int(height * scale_factor)
                pil_image = pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
            else:
                pil_image = pil_image.resize((width, height), Image.Resampling.LANCZOS)

            # è®¡ç®—éœ€è¦çš„å¸§æ•°
            total_frames = int(duration * fps)
            content_frames = total_frames - (transition_frames if i < len(images) - 1 else 0)

            print(f"ğŸ“¹ åœºæ™¯ {i+1}: ç”Ÿæˆ {total_frames} å¸§ ({duration:.2f}ç§’)")
            print(f"   å†…å®¹å¸§: {content_frames}, è½¬åœºå¸§: {transition_frames if i < len(images) - 1 else 0}")

            # ç”ŸæˆåŠ¨ç”»å¸§
            scene_frames = self._generate_scene_animation_frames(
                pil_image, content_frames, width, height, animation_type, i
            )
            all_frames.extend(scene_frames)

            # æ·»åŠ è½¬åœºæ•ˆæœï¼ˆé™¤äº†æœ€åä¸€ä¸ªåœºæ™¯ï¼‰
            if i < len(images) - 1 and transition_frames > 0:
                next_image_tensor = images[i + 1]
                next_image_np = (next_image_tensor.cpu().numpy() * 255).astype(np.uint8)
                next_pil_image = Image.fromarray(next_image_np)

                if animation_type == "coze_zoom":
                    next_pil_image = next_pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
                else:
                    next_pil_image = next_pil_image.resize((width, height), Image.Resampling.LANCZOS)

                transition_frames_list = self._generate_transition_frames(
                    pil_image, next_pil_image, transition_frames, width, height, animation_type
                )
                all_frames.extend(transition_frames_list)

        return all_frames

    def _create_animated_frame_generator(self, images, durations, fps, width, height,
                                       animation_type, transition_duration) -> Generator[np.ndarray, None, None]:
        """
        åˆ›å»ºå¸¦åŠ¨ç”»æ•ˆæœçš„å¸§ç”Ÿæˆå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        ä½¿ç”¨Generatoræ¨¡å¼ï¼Œæ¯æ¬¡åªç”Ÿæˆä¸€å¸§ï¼Œé¿å…å†…å­˜æº¢å‡º
        """
        transition_frames = int(transition_duration * fps) if transition_duration > 0 else 0

        for i, (image_tensor, duration) in enumerate(zip(images, durations)):
            print(f"ğŸ“¹ å¤„ç†åœºæ™¯ {i+1}/{len(images)}")

            # è½¬æ¢tensoråˆ°PILå›¾ç‰‡
            image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(image_np)

            # è°ƒæ•´å›¾ç‰‡å°ºå¯¸åˆ°ç•¥å¤§äºç›®æ ‡ï¼Œç”¨äºç¼©æ”¾åŠ¨ç”»
            if animation_type == "coze_zoom":
                scale_factor = 1.3
                scaled_width = int(width * scale_factor)
                scaled_height = int(height * scale_factor)
                pil_image = pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
            else:
                pil_image = pil_image.resize((width, height), Image.Resampling.LANCZOS)

            # è®¡ç®—éœ€è¦çš„å¸§æ•°
            total_frames = int(duration * fps)
            content_frames = total_frames - (transition_frames if i < len(images) - 1 else 0)

            # ç”Ÿæˆåœºæ™¯åŠ¨ç”»å¸§
            for frame in self._generate_scene_animation_frames_generator(
                pil_image, content_frames, width, height, animation_type, i
            ):
                yield frame

            # ç”Ÿæˆè½¬åœºå¸§ï¼ˆé™¤äº†æœ€åä¸€ä¸ªåœºæ™¯ï¼‰
            if i < len(images) - 1 and transition_frames > 0:
                next_image_tensor = images[i + 1]
                next_image_np = (next_image_tensor.cpu().numpy() * 255).astype(np.uint8)
                next_pil_image = Image.fromarray(next_image_np)

                if animation_type == "coze_zoom":
                    next_pil_image = next_pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
                else:
                    next_pil_image = next_pil_image.resize((width, height), Image.Resampling.LANCZOS)

                # è®¡ç®—è½¬åœºæ—¶çš„ç¼©æ”¾è¿ç»­æ€§
                current_scene_is_odd = i % 2 == 0
                next_scene_is_odd = (i + 1) % 2 == 0

                # å½“å‰åœºæ™¯ç»“æŸæ—¶çš„ç¼©æ”¾å€¼
                current_end_scale = 1.5 if current_scene_is_odd else 1.0
                # ä¸‹ä¸€åœºæ™¯å¼€å§‹æ—¶çš„ç¼©æ”¾å€¼
                next_start_scale = 1.0 if next_scene_is_odd else 1.5

                print(f"ğŸ”„ è½¬åœº {i+1}â†’{i+2}: {current_end_scale:.1f} â†’ {next_start_scale:.1f}")

                for frame in self._generate_transition_frames_generator(
                    pil_image, next_pil_image, transition_frames, width, height,
                    animation_type, current_end_scale, next_start_scale
                ):
                    yield frame

    def _generate_scene_animation_frames_generator(self, pil_image, frame_count, width, height,
                                                 animation_type, scene_index) -> Generator[np.ndarray, None, None]:
        """åœºæ™¯åŠ¨ç”»å¸§ç”Ÿæˆå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""

        if animation_type == "coze_zoom":
            # Cozeé£æ ¼ç¼©æ”¾åŠ¨ç”»
            is_odd_scene = scene_index % 2 == 0

            if is_odd_scene:
                start_scale, end_scale = 1.0, 1.5
            else:
                start_scale, end_scale = 1.5, 1.0

            for frame_idx in range(frame_count):
                progress = frame_idx / max(frame_count - 1, 1)
                eased_progress = self._ease_in_out(progress)
                current_scale = start_scale + (end_scale - start_scale) * eased_progress

                # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100å¸§è¾“å‡ºä¸€æ¬¡å½“å‰ç¼©æ”¾å€¼
                if frame_idx % 100 == 0:
                    print(f"     å¸§{frame_idx}: scale={current_scale:.3f} (progress={progress:.3f})")

                frame = self._apply_zoom_effect(pil_image, current_scale, width, height)
                yield frame

        elif animation_type == "fade":
            # æ·¡å…¥æ•ˆæœ
            base_frame = np.array(pil_image.resize((width, height), Image.Resampling.LANCZOS))
            base_frame_bgr = cv2.cvtColor(base_frame, cv2.COLOR_RGB2BGR)

            for frame_idx in range(frame_count):
                progress = min(frame_idx / (frame_count * 0.2), 1.0)
                alpha = progress
                frame = base_frame_bgr * alpha
                yield frame.astype(np.uint8)

        else:  # none æˆ–å…¶ä»–
            # é™æ€å¸§
            base_frame = np.array(pil_image.resize((width, height), Image.Resampling.LANCZOS))
            base_frame_bgr = cv2.cvtColor(base_frame, cv2.COLOR_RGB2BGR)

            for _ in range(frame_count):
                yield base_frame_bgr

    def _generate_transition_frames_generator(self, current_image, next_image, frame_count,
                                            width, height, animation_type,
                                            current_end_scale=1.0, next_start_scale=1.0) -> Generator[np.ndarray, None, None]:
        """è½¬åœºå¸§ç”Ÿæˆå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒç¼©æ”¾è¿ç»­æ€§ï¼‰"""

        if animation_type == "coze_zoom":
            # Cozeç¼©æ”¾è½¬åœºï¼šä¿æŒç¼©æ”¾è¿ç»­æ€§çš„äº¤å‰æ·¡åŒ–
            for frame_idx in range(frame_count):
                progress = frame_idx / max(frame_count - 1, 1)
                alpha = self._ease_in_out(progress)

                # å½“å‰å¸§ï¼šä»ç»“æŸç¼©æ”¾æ¸å˜åˆ°1.0
                current_scale = current_end_scale + (1.0 - current_end_scale) * alpha
                current_frame = self._apply_zoom_effect(current_image, current_scale, width, height)

                # ä¸‹ä¸€å¸§ï¼šä»1.0æ¸å˜åˆ°å¼€å§‹ç¼©æ”¾
                next_scale = 1.0 + (next_start_scale - 1.0) * alpha
                next_frame = self._apply_zoom_effect(next_image, next_scale, width, height)

                # äº¤å‰æ·¡åŒ–
                current_frame_float = current_frame.astype(np.float32)
                next_frame_float = next_frame.astype(np.float32)

                blended = current_frame_float * (1 - alpha) + next_frame_float * alpha
                yield blended.astype(np.uint8)

        elif animation_type == "fade":
            # ç®€å•æ·¡åŒ–è½¬åœº
            current_frame = np.array(current_image.resize((width, height), Image.Resampling.LANCZOS))
            next_frame = np.array(next_image.resize((width, height), Image.Resampling.LANCZOS))

            current_bgr = cv2.cvtColor(current_frame, cv2.COLOR_RGB2BGR)
            next_bgr = cv2.cvtColor(next_frame, cv2.COLOR_RGB2BGR)

            for frame_idx in range(frame_count):
                progress = frame_idx / max(frame_count - 1, 1)
                alpha = progress
                blended = current_bgr * (1 - alpha) + next_bgr * alpha
                yield blended.astype(np.uint8)

        else:
            # ç¡¬åˆ‡æ¢è½¬åœº
            current_frame = np.array(current_image.resize((width, height), Image.Resampling.LANCZOS))
            next_frame = np.array(next_image.resize((width, height), Image.Resampling.LANCZOS))

            current_bgr = cv2.cvtColor(current_frame, cv2.COLOR_RGB2BGR)
            next_bgr = cv2.cvtColor(next_frame, cv2.COLOR_RGB2BGR)

            for frame_idx in range(frame_count):
                progress = frame_idx / max(frame_count - 1, 1)
                if progress < 0.5:
                    yield current_bgr
                else:
                    yield next_bgr

    def _generate_scene_animation_frames(self, pil_image, frame_count, width, height,
                                       animation_type, scene_index):
        """
        ä¸ºå•ä¸ªåœºæ™¯ç”ŸæˆåŠ¨ç”»å¸§

        Cozeå·¥ä½œæµçš„æ ¸å¿ƒåŠ¨ç”»ï¼š
        - å¥‡æ•°åœºæ™¯ï¼šä»1.0ç¼©æ”¾åˆ°1.5
        - å¶æ•°åœºæ™¯ï¼šä»1.5ç¼©æ”¾åˆ°1.0
        """
        frames = []

        if animation_type == "coze_zoom":
            # Cozeé£æ ¼ç¼©æ”¾åŠ¨ç”»
            is_odd_scene = scene_index % 2 == 0  # 0-indexedï¼Œæ‰€ä»¥å¶æ•°ç´¢å¼•æ˜¯å¥‡æ•°åœºæ™¯

            if is_odd_scene:
                # å¥‡æ•°åœºæ™¯ï¼š1.0 â†’ 1.5ï¼ˆæ”¾å¤§ï¼‰
                start_scale = 1.0
                end_scale = 1.5
            else:
                # å¶æ•°åœºæ™¯ï¼š1.5 â†’ 1.0ï¼ˆç¼©å°ï¼‰
                start_scale = 1.5
                end_scale = 1.0

            print(f"   Cozeç¼©æ”¾: {start_scale:.1f} â†’ {end_scale:.1f}")

            for frame_idx in range(frame_count):
                # è®¡ç®—å½“å‰å¸§çš„ç¼©æ”¾æ¯”ä¾‹ï¼ˆä½¿ç”¨ç¼“åŠ¨å‡½æ•°ï¼‰
                progress = frame_idx / max(frame_count - 1, 1)
                # ä½¿ç”¨ease-in-outç¼“åŠ¨
                eased_progress = self._ease_in_out(progress)
                current_scale = start_scale + (end_scale - start_scale) * eased_progress

                # åº”ç”¨ç¼©æ”¾æ•ˆæœ
                frame = self._apply_zoom_effect(pil_image, current_scale, width, height)
                frames.append(frame)

        elif animation_type == "fade":
            # æ·¡å…¥æ•ˆæœ
            base_frame = np.array(pil_image.resize((width, height), Image.Resampling.LANCZOS))
            base_frame_bgr = cv2.cvtColor(base_frame, cv2.COLOR_RGB2BGR)

            for frame_idx in range(frame_count):
                progress = min(frame_idx / (frame_count * 0.2), 1.0)  # å‰20%æ—¶é—´æ·¡å…¥
                alpha = progress
                frame = base_frame_bgr * alpha
                frames.append(frame.astype(np.uint8))

        else:  # none æˆ–å…¶ä»–
            # é™æ€å¸§
            base_frame = np.array(pil_image.resize((width, height), Image.Resampling.LANCZOS))
            base_frame_bgr = cv2.cvtColor(base_frame, cv2.COLOR_RGB2BGR)

            for _ in range(frame_count):
                frames.append(base_frame_bgr)

        return frames

    def _apply_zoom_effect(self, pil_image, scale, target_width, target_height):
        """
        åº”ç”¨Ken Burnsç¼©æ”¾æ•ˆæœ

        scale = 1.0: æ˜¾ç¤ºå®Œæ•´å›¾ç‰‡
        scale = 1.5: æ”¾å¤§1.5å€ï¼ˆæ˜¾ç¤ºä¸­å¿ƒ2/3åŒºåŸŸï¼‰
        scale = 2.0: æ”¾å¤§2å€ï¼ˆæ˜¾ç¤ºä¸­å¿ƒ1/2åŒºåŸŸï¼‰

        ç®—æ³•ï¼šä½¿ç”¨åå‘æ€ç»´ï¼Œå°†åŸå›¾çœ‹ä½œ"è™šæ‹Ÿç”»å¸ƒ"ï¼Œä»ä¸­è£å‰ªæŒ‡å®šæ¯”ä¾‹
        """
        img_width, img_height = pil_image.size

        # Ken Burnsæ•ˆæœï¼šscaleè¶Šå¤§ï¼Œçœ‹åˆ°çš„å›¾ç‰‡åŒºåŸŸè¶Šå°ï¼ˆæ”¾å¤§æ•ˆæœï¼‰
        visible_ratio = 1.0 / scale

        # è®¡ç®—å¯è§åŒºåŸŸå°ºå¯¸
        visible_width = img_width * visible_ratio
        visible_height = img_height * visible_ratio

        # è½¬æ¢ä¸ºæ•´æ•°åƒç´ 
        crop_width = max(1, int(visible_width))
        crop_height = max(1, int(visible_height))

        # ç¡®ä¿ä¸è¶…å‡ºåŸå›¾è¾¹ç•Œ
        crop_width = min(crop_width, img_width)
        crop_height = min(crop_height, img_height)

        # å±…ä¸­è£å‰ª
        center_x = img_width // 2
        center_y = img_height // 2

        left = center_x - crop_width // 2
        top = center_y - crop_height // 2
        right = left + crop_width
        bottom = top + crop_height

        # è¾¹ç•Œæ£€æŸ¥
        if left < 0:
            right = right - left
            left = 0
        if right > img_width:
            left = left - (right - img_width)
            right = img_width

        if top < 0:
            bottom = bottom - top
            top = 0
        if bottom > img_height:
            top = top - (bottom - img_height)
            bottom = img_height

        # è£å‰ªå¹¶ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
        cropped = pil_image.crop((left, top, right, bottom))
        resized = cropped.resize((target_width, target_height), Image.Resampling.LANCZOS)

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        frame_np = np.array(resized)
        frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)

        return frame_bgr

    def _generate_transition_frames(self, current_image, next_image, frame_count,
                                  width, height, animation_type):
        """ç”Ÿæˆè½¬åœºå¸§"""
        frames = []

        current_frame = np.array(current_image.resize((width, height), Image.Resampling.LANCZOS))
        next_frame = np.array(next_image.resize((width, height), Image.Resampling.LANCZOS))

        current_bgr = cv2.cvtColor(current_frame, cv2.COLOR_RGB2BGR)
        next_bgr = cv2.cvtColor(next_frame, cv2.COLOR_RGB2BGR)

        for frame_idx in range(frame_count):
            progress = frame_idx / max(frame_count - 1, 1)

            if animation_type == "fade":
                # äº¤å‰æ·¡åŒ–
                alpha = progress
                blended = current_bgr * (1 - alpha) + next_bgr * alpha
                frames.append(blended.astype(np.uint8))
            else:
                # é»˜è®¤ï¼šå¿«é€Ÿåˆ‡æ¢ï¼ˆç”¨äºå…¶ä»–åŠ¨ç”»ç±»å‹ï¼‰
                if progress < 0.5:
                    frames.append(current_bgr)
                else:
                    frames.append(next_bgr)

        return frames

    def _ease_in_out(self, t):
        """ç¼“åŠ¨å‡½æ•°ï¼šease-in-out"""
        return t * t * (3.0 - 2.0 * t)

    def _merge_video_audio_streaming(self, frame_generator, audio_path, output_path, fps, quality, total_frames):
        """ä½¿ç”¨æµå¼å¤„ç†åˆæˆè§†é¢‘ï¼Œé¿å…å†…å­˜æº¢å‡º"""
        temp_video_path = output_path + ".temp.mp4"

        try:
            # è®¾ç½®è§†é¢‘ç¼–ç å™¨å‚æ•°
            if quality == "high":
                crf = 18
            elif quality == "medium":
                crf = 23
            else:  # low
                crf = 28

            print(f"ğŸ¬ å¼€å§‹æµå¼å†™å…¥è§†é¢‘: {total_frames} å¸§")

            # è·å–ç¬¬ä¸€å¸§ç¡®å®šè§†é¢‘å°ºå¯¸
            first_frame = next(frame_generator)
            height, width = first_frame.shape[:2]

            # ä½¿ç”¨OpenCVåˆ›å»ºè§†é¢‘å†™å…¥å™¨
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))

            if not out.isOpened():
                raise Exception("æ— æ³•åˆ›å»ºè§†é¢‘å†™å…¥å™¨")

            # å†™å…¥ç¬¬ä¸€å¸§
            out.write(first_frame)
            frames_written = 1

            # æµå¼å†™å…¥å‰©ä½™å¸§
            for frame in frame_generator:
                out.write(frame)
                frames_written += 1

                # æ¯1000å¸§æŠ¥å‘Šè¿›åº¦
                if frames_written % 1000 == 0:
                    progress = (frames_written / total_frames) * 100
                    print(f"ğŸ“¹ å†™å…¥è¿›åº¦: {frames_written}/{total_frames} ({progress:.1f}%)")

            out.release()
            print(f"âœ… è§†é¢‘å†™å…¥å®Œæˆ: {frames_written} å¸§")

            # ä½¿ç”¨ffmpegæ·»åŠ éŸ³é¢‘
            self._add_audio_with_ffmpeg(temp_video_path, audio_path, output_path, crf)

        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_video_path):
                os.unlink(temp_video_path)
            raise e

    def _add_audio_with_ffmpeg(self, video_path, audio_path, output_path, crf):
        """ä½¿ç”¨ffmpegæ·»åŠ éŸ³é¢‘"""
        try:
            # ffmpegå‘½ä»¤ - ä½¿ç”¨æ›´å¥½çš„ç¼–ç å‚æ•°
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-i', audio_path,
                '-c:v', 'libx264',
                '-crf', str(crf),
                '-preset', 'medium',
                '-c:a', 'aac',
                '-b:a', '128k',
                '-shortest',  # ä»¥æœ€çŸ­çš„æµä¸ºå‡†
                output_path
            ]

            print("ğŸ”Š æ·»åŠ éŸ³é¢‘è½¨é“...")
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)

            # æ¸…ç†ä¸´æ—¶è§†é¢‘æ–‡ä»¶
            os.unlink(video_path)
            print("ğŸµ éŸ³é¢‘åˆå¹¶æˆåŠŸ")

        except subprocess.CalledProcessError as e:
            print(f"âš ï¸ ffmpegæ‰§è¡Œå¤±è´¥: {e.stderr}")
            # å¦‚æœffmpegå¤±è´¥ï¼Œä½¿ç”¨åŸè§†é¢‘
            shutil.move(video_path, output_path)
            print("ğŸ“¹ ä½¿ç”¨æ— éŸ³é¢‘è§†é¢‘")

        except FileNotFoundError:
            print("âš ï¸ ffmpegæœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ— éŸ³é¢‘è§†é¢‘")
            shutil.move(video_path, output_path)

    def _get_timestamp(self):
        """è·å–æ—¶é—´æˆ³"""
        import time
        return str(int(time.time()))


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "VideoComposer": VideoComposer
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoComposer": "ğŸ¬ è§†é¢‘åˆæˆå™¨"
}