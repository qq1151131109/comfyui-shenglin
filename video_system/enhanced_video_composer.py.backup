"""
å¢å¼ºç‰ˆè§†é¢‘åˆæˆå™¨èŠ‚ç‚¹ - é›†æˆéŸ³æ•ˆåº“
åŸºäºåŸVideoComposerï¼Œæ–°å¢3è½¨éŸ³é¢‘ç³»ç»Ÿæ”¯æŒ
"""

import os
import sys
import tempfile
import subprocess
import shutil
import torch
import torchaudio
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import cv2
import folder_paths
from typing import List, Dict, Any, Tuple, Generator, Optional
import json
import math

# å¯¼å…¥éŸ³æ•ˆç®¡ç†å™¨ï¼ˆä»å½“å‰èŠ‚ç‚¹ç›®å½•ï¼‰
try:
    # å°è¯•ä»èŠ‚ç‚¹ç›®å½•å¯¼å…¥
    current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, current_dir)
    from audio_effects_manager import AudioEffectsManager
    AUDIO_EFFECTS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ éŸ³æ•ˆåº“ä¸å¯ç”¨: {e}")
    AUDIO_EFFECTS_AVAILABLE = False

class EnhancedVideoComposer:
    """
    å¢å¼ºç‰ˆè§†é¢‘åˆæˆå™¨ - æ”¯æŒ3è½¨éŸ³é¢‘ç³»ç»Ÿ

    1. ä¸»éŸ³è½¨ï¼šTTSç”Ÿæˆçš„æ—ç™½è¯­éŸ³
    2. èƒŒæ™¯éŸ³ä¹è½¨ï¼šå¾ªç¯æ’­æ”¾çš„èƒŒæ™¯éŸ³ä¹ï¼ˆéŸ³é‡30%ï¼‰
    3. éŸ³æ•ˆè½¨ï¼šå¼€åœºéŸ³æ•ˆç­‰ç‰¹æ®ŠéŸ³æ•ˆï¼ˆéŸ³é‡80%ï¼‰
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_list": ("*", {"tooltip": "éŸ³é¢‘åˆ—è¡¨ï¼Œæ¯ä¸ªéŸ³é¢‘å¯¹åº”ä¸€ä¸ªåœºæ™¯"}),
                "images": ("IMAGE", {"tooltip": "å›¾ç‰‡batchï¼Œæ¯å¼ å›¾ç‰‡å¯¹åº”ä¸€ä¸ªåœºæ™¯"}),
                "fps": ("INT", {
                    "default": 30,
                    "min": 15,
                    "max": 60,
                    "step": 1,
                    "tooltip": "è§†é¢‘å¸§ç‡"
                }),
                "width": ("INT", {
                    "default": 720,
                    "min": 480,
                    "max": 1920,
                    "step": 8,
                    "tooltip": "è§†é¢‘å®½åº¦"
                }),
                "height": ("INT", {
                    "default": 1280,
                    "min": 720,
                    "max": 2560,
                    "step": 8,
                    "tooltip": "è§†é¢‘é«˜åº¦"
                })
            },
            "optional": {
                "output_format": (["mp4", "avi", "mov"], {
                    "default": "mp4",
                    "tooltip": "è¾“å‡ºè§†é¢‘æ ¼å¼"
                }),
                "quality": (["high", "medium", "low"], {
                    "default": "medium",
                    "tooltip": "è§†é¢‘è´¨é‡"
                }),
                "animation_type": (["coze_zoom", "fade", "slide", "none"], {
                    "default": "coze_zoom",
                    "tooltip": "åŠ¨ç”»æ•ˆæœç±»å‹"
                }),
                "transition_duration": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "è½¬åœºæ—¶é•¿ï¼ˆç§’ï¼‰"
                }),

                # ğŸµ æ–°å¢éŸ³æ•ˆåº“å‚æ•°
                "enable_audio_effects": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨éŸ³æ•ˆåº“ï¼ˆèƒŒæ™¯éŸ³ä¹+å¼€åœºéŸ³æ•ˆï¼‰"
                }),
                "background_music_volume": ("FLOAT", {
                    "default": 0.3,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "èƒŒæ™¯éŸ³ä¹éŸ³é‡ï¼ˆ0.3=30%ï¼‰"
                }),
                "opening_sound_volume": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "å¼€åœºéŸ³æ•ˆéŸ³é‡ï¼ˆ0.8=80%ï¼‰"
                }),
                "voice_volume": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "ä¸»éŸ³è½¨ï¼ˆè¯­éŸ³ï¼‰éŸ³é‡"
                }),

                # ä¸»è§’å›¾ç›¸å…³å‚æ•°
                "character_image": ("IMAGE", {
                    "tooltip": "ä¸»è§’å›¾ç‰‡ï¼Œç”¨äºé¦–å¸§ç‰¹æ•ˆï¼ˆå¯é€‰ï¼‰"
                }),
                "enable_character_intro": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨ä¸»è§’å¼€åœºåŠ¨ç”»"
                }),
                "char_intro_scale_start": ("FLOAT", {
                    "default": 2.0,
                    "min": 0.5,
                    "max": 5.0,
                    "step": 0.1,
                    "tooltip": "ä¸»è§’å›¾å¼€å§‹ç¼©æ”¾æ¯”ä¾‹"
                }),
                "char_intro_scale_mid": ("FLOAT", {
                    "default": 1.2,
                    "min": 0.5,
                    "max": 3.0,
                    "step": 0.1,
                    "tooltip": "ä¸»è§’å›¾ä¸­é—´ç¼©æ”¾æ¯”ä¾‹"
                }),
                "char_intro_scale_end": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.5,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "ä¸»è§’å›¾ç»“æŸç¼©æ”¾æ¯”ä¾‹"
                }),
                "char_intro_mid_timing": ("FLOAT", {
                    "default": 0.533,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "ä¸»è§’å›¾ä¸­é—´å…³é”®å¸§æ—¶é—´ç‚¹ï¼ˆç§’ï¼‰"
                })
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("video_path", "info")
    FUNCTION = "compose_video_with_effects"
    CATEGORY = "ğŸ”¥ Shenglin/è§†é¢‘å¤„ç†"
    DESCRIPTION = "å¢å¼ºç‰ˆè§†é¢‘åˆæˆå™¨ï¼Œæ”¯æŒ3è½¨éŸ³é¢‘ç³»ç»Ÿï¼ˆè¯­éŸ³+èƒŒæ™¯éŸ³ä¹+éŸ³æ•ˆï¼‰"

    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()

        # åˆå§‹åŒ–éŸ³æ•ˆç®¡ç†å™¨
        if AUDIO_EFFECTS_AVAILABLE:
            try:
                self.audio_effects_manager = AudioEffectsManager()
                print("âœ… éŸ³æ•ˆåº“åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ éŸ³æ•ˆåº“åˆå§‹åŒ–å¤±è´¥: {e}")
                self.audio_effects_manager = None
        else:
            self.audio_effects_manager = None

    def compose_video_with_effects(self, audio_list, images, fps=30, width=720, height=1280,
                                 output_format="mp4", quality="medium", animation_type="coze_zoom",
                                 transition_duration=0.5, enable_audio_effects=True,
                                 background_music_volume=0.3, opening_sound_volume=0.8, voice_volume=1.0,
                                 character_image=None, enable_character_intro=False,
                                 char_intro_scale_start=2.0, char_intro_scale_mid=1.2,
                                 char_intro_scale_end=1.0, char_intro_mid_timing=0.533):
        """
        å¢å¼ºç‰ˆè§†é¢‘åˆæˆ - æ”¯æŒ3è½¨éŸ³é¢‘ç³»ç»Ÿ
        """
        try:
            # æ£€æŸ¥è¾“å…¥
            if not isinstance(audio_list, list):
                raise ValueError("audio_listå¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹")

            if len(audio_list) == 0:
                raise ValueError("éŸ³é¢‘åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

            if images.shape[0] != len(audio_list):
                print(f"âš ï¸ è­¦å‘Šï¼šå›¾ç‰‡æ•°é‡({images.shape[0]}) ä¸éŸ³é¢‘æ•°é‡({len(audio_list)}) ä¸åŒ¹é…")
                min_count = min(images.shape[0], len(audio_list))
                images = images[:min_count]
                audio_list = audio_list[:min_count]

            print(f"ğŸ¬ å¼€å§‹å¢å¼ºç‰ˆè§†é¢‘åˆæˆï¼š{len(audio_list)} ä¸ªåœºæ™¯ï¼Œåˆ†è¾¨ç‡ {width}x{height}")
            if enable_audio_effects:
                print("ğŸµ å¯ç”¨3è½¨éŸ³é¢‘ç³»ç»Ÿï¼ˆè¯­éŸ³+èƒŒæ™¯éŸ³ä¹+éŸ³æ•ˆï¼‰")

            # 1. åˆ†æéŸ³é¢‘æ—¶é•¿
            audio_durations = []
            total_duration = 0

            for i, audio_dict in enumerate(audio_list):
                if isinstance(audio_dict, dict) and "waveform" in audio_dict:
                    waveform = audio_dict["waveform"]
                    sample_rate = audio_dict["sample_rate"]

                    if len(waveform.shape) == 3:
                        waveform = waveform[0]  # ç§»é™¤batchç»´åº¦

                    duration = waveform.shape[1] / sample_rate
                    audio_durations.append(duration)
                    total_duration += duration
                    print(f"ğŸµ åœºæ™¯ {i+1} éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")
                else:
                    raise ValueError(f"éŸ³é¢‘ {i} æ ¼å¼ä¸æ­£ç¡®ï¼Œéœ€è¦åŒ…å«waveformå’Œsample_rate")

            # 2. åˆæˆ3è½¨éŸ³é¢‘ç³»ç»Ÿ
            print("ğŸ”Š å¼€å§‹3è½¨éŸ³é¢‘åˆæˆ...")
            if enable_audio_effects and self.audio_effects_manager:
                combined_audio_path = self._combine_audio_with_effects(
                    audio_list, total_duration, background_music_volume,
                    opening_sound_volume, voice_volume
                )
            else:
                # å›é€€åˆ°åŸå§‹å•è½¨éŸ³é¢‘
                print("ğŸ“¢ ä½¿ç”¨å•è½¨éŸ³é¢‘æ¨¡å¼")
                combined_audio_path = self._combine_audio_simple(audio_list, voice_volume)

            # 3. è®¡ç®—æ€»å¸§æ•°
            total_frames = sum(int(duration * fps) for duration in audio_durations)
            transition_frames_total = int(transition_duration * fps) * (len(images) - 1) if transition_duration > 0 else 0
            total_frames += transition_frames_total

            print(f"ğŸ“Š é¢„è®¡ç”Ÿæˆ {total_frames} å¸§")

            # 4. åˆæˆæœ€ç»ˆè§†é¢‘
            print("ğŸ¬ å¼€å§‹è§†é¢‘åˆæˆ...")
            output_filename = f"story_video_enhanced_{self._get_timestamp()}.{output_format}"
            video_path = os.path.join(self.output_dir, output_filename)

            # åˆ›å»ºå¸§ç”Ÿæˆå™¨
            from .video_composer import VideoComposer  # å¯¼å…¥åŸå§‹ç±»çš„æ–¹æ³•
            original_composer = VideoComposer()

            frame_generator = original_composer._create_animated_frame_generator(
                images, audio_durations, fps, width, height,
                animation_type, transition_duration, character_image, enable_character_intro,
                char_intro_scale_start, char_intro_scale_mid, char_intro_scale_end, char_intro_mid_timing
            )

            original_composer._merge_video_audio_streaming(
                frame_generator, combined_audio_path, video_path, fps, quality, total_frames
            )

            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(combined_audio_path)
            except:
                pass

            # 6. ç”Ÿæˆä¿¡æ¯æŠ¥å‘Š
            audio_tracks_info = "3è½¨éŸ³é¢‘" if (enable_audio_effects and self.audio_effects_manager) else "å•è½¨éŸ³é¢‘"
            character_info = "å·²å¯ç”¨" if enable_character_intro else "æœªå¯ç”¨"

            info = f"""ğŸ¬ å¢å¼ºç‰ˆè§†é¢‘åˆæˆå®Œæˆ
ğŸ“ è¾“å‡ºè·¯å¾„: {video_path}
ğŸ“Š è§†é¢‘è§„æ ¼: {width}x{height}@{fps}fps
ğŸµ éŸ³é¢‘ç³»ç»Ÿ: {audio_tracks_info}
ğŸ‘¤ ä¸»è§’åŠ¨ç”»: {character_info}
ğŸ“ˆ æ€»æ—¶é•¿: {total_duration:.2f}ç§’
ğŸï¸ æ€»å¸§æ•°: {total_frames}"""

            print("âœ… å¢å¼ºç‰ˆè§†é¢‘åˆæˆå®Œæˆï¼")
            return (video_path, info)

        except Exception as e:
            error_msg = f"âŒ å¢å¼ºç‰ˆè§†é¢‘åˆæˆå¤±è´¥: {str(e)}"
            print(error_msg)
            return ("", error_msg)

    def _combine_audio_with_effects(self, audio_list, total_duration, bg_volume, opening_volume, voice_volume):
        """
        3è½¨éŸ³é¢‘åˆæˆï¼šè¯­éŸ³ + èƒŒæ™¯éŸ³ä¹ + å¼€åœºéŸ³æ•ˆ
        """
        try:
            # 1. åˆæˆä¸»éŸ³è½¨ï¼ˆè¯­éŸ³ï¼‰
            voice_waveforms = []
            sample_rate = None

            for audio_dict in audio_list:
                waveform = audio_dict["waveform"]
                if len(waveform.shape) == 3:
                    waveform = waveform[0]  # ç§»é™¤batchç»´åº¦
                voice_waveforms.append(waveform)
                sample_rate = audio_dict["sample_rate"]

            # æ‹¼æ¥è¯­éŸ³
            voice_combined = torch.cat(voice_waveforms, dim=1)
            # åº”ç”¨è¯­éŸ³éŸ³é‡
            voice_combined = voice_combined * voice_volume

            print(f"ğŸ¤ ä¸»éŸ³è½¨ï¼š{voice_combined.shape[1]/sample_rate:.2f}ç§’ï¼ŒéŸ³é‡{voice_volume*100:.0f}%")

            # 2. åŠ è½½èƒŒæ™¯éŸ³ä¹
            bg_music_path = self.audio_effects_manager.get_background_music()
            if bg_music_path:
                bg_waveform, bg_sample_rate = torchaudio.load(bg_music_path)

                # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
                if bg_sample_rate != sample_rate:
                    resampler = torchaudio.transforms.Resample(bg_sample_rate, sample_rate)
                    bg_waveform = resampler(bg_waveform)

                # å¾ªç¯èƒŒæ™¯éŸ³ä¹ä»¥åŒ¹é…æ€»æ—¶é•¿
                bg_length_needed = int(total_duration * sample_rate)
                bg_current_length = bg_waveform.shape[1]

                if bg_length_needed > bg_current_length:
                    # éœ€è¦å¾ªç¯
                    repeats = (bg_length_needed // bg_current_length) + 1
                    bg_waveform = bg_waveform.repeat(1, repeats)

                # è£å‰ªåˆ°ç²¾ç¡®é•¿åº¦
                bg_waveform = bg_waveform[:, :bg_length_needed]

                # åº”ç”¨èƒŒæ™¯éŸ³ä¹éŸ³é‡
                bg_waveform = bg_waveform * bg_volume

                # è½¬æ¢ä¸ºå•å£°é“ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if bg_waveform.shape[0] == 2:
                    bg_waveform = bg_waveform.mean(dim=0, keepdim=True)

                print(f"ğŸ¼ èƒŒæ™¯éŸ³ä¹ï¼š{bg_waveform.shape[1]/sample_rate:.2f}ç§’ï¼ŒéŸ³é‡{bg_volume*100:.0f}%")
            else:
                # åˆ›å»ºé™éŸ³èƒŒæ™¯
                bg_waveform = torch.zeros(1, voice_combined.shape[1])
                print("ğŸ”‡ èƒŒæ™¯éŸ³ä¹ï¼šæ— å¯ç”¨æ–‡ä»¶ï¼Œä½¿ç”¨é™éŸ³")

            # 3. åŠ è½½å¼€åœºéŸ³æ•ˆ
            opening_path = self.audio_effects_manager.get_opening_sound()
            if opening_path:
                opening_waveform, opening_sample_rate = torchaudio.load(opening_path)

                # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
                if opening_sample_rate != sample_rate:
                    resampler = torchaudio.transforms.Resample(opening_sample_rate, sample_rate)
                    opening_waveform = resampler(opening_waveform)

                # åº”ç”¨å¼€åœºéŸ³æ•ˆéŸ³é‡
                opening_waveform = opening_waveform * opening_volume

                # è½¬æ¢ä¸ºå•å£°é“ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if opening_waveform.shape[0] == 2:
                    opening_waveform = opening_waveform.mean(dim=0, keepdim=True)

                # åˆ›å»ºå¼€åœºéŸ³æ•ˆè½¨ï¼ˆåªåœ¨å¼€å¤´æ’­æ”¾ï¼‰
                opening_track = torch.zeros(1, voice_combined.shape[1])
                opening_length = min(opening_waveform.shape[1], opening_track.shape[1])
                opening_track[:, :opening_length] = opening_waveform[:, :opening_length]

                print(f"ğŸº å¼€åœºéŸ³æ•ˆï¼š{opening_waveform.shape[1]/sample_rate:.2f}ç§’ï¼ŒéŸ³é‡{opening_volume*100:.0f}%")
            else:
                opening_track = torch.zeros(1, voice_combined.shape[1])
                print("ğŸ”‡ å¼€åœºéŸ³æ•ˆï¼šæ— å¯ç”¨æ–‡ä»¶ï¼Œä½¿ç”¨é™éŸ³")

            # 4. æ··åˆ3è½¨éŸ³é¢‘
            # ç¡®ä¿æ‰€æœ‰è½¨é“é•¿åº¦ä¸€è‡´
            min_length = min(voice_combined.shape[1], bg_waveform.shape[1], opening_track.shape[1])

            voice_combined = voice_combined[:, :min_length]
            bg_waveform = bg_waveform[:, :min_length]
            opening_track = opening_track[:, :min_length]

            # æ··åˆéŸ³é¢‘
            final_audio = voice_combined + bg_waveform + opening_track

            # é˜²æ­¢éŸ³é¢‘å‰ªåˆ‡ï¼ˆç®€å•å‹ç¼©ï¼‰
            max_val = torch.max(torch.abs(final_audio))
            if max_val > 1.0:
                final_audio = final_audio / max_val * 0.95
                print(f"ğŸ”§ éŸ³é¢‘å‹ç¼©ï¼šå³°å€¼ä»{max_val:.2f}å‹ç¼©åˆ°0.95")

            # 5. ä¿å­˜æ··åˆéŸ³é¢‘
            temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
            torchaudio.save(temp_file.name, final_audio, sample_rate)
            temp_file.close()

            print(f"âœ… 3è½¨éŸ³é¢‘åˆæˆå®Œæˆï¼š{final_audio.shape[1]/sample_rate:.2f}ç§’")
            return temp_file.name

        except Exception as e:
            print(f"âŒ 3è½¨éŸ³é¢‘åˆæˆå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•éŸ³é¢‘åˆæˆ
            return self._combine_audio_simple(audio_list, voice_volume)

    def _combine_audio_simple(self, audio_list, voice_volume=1.0):
        """ç®€å•éŸ³é¢‘åˆæˆï¼ˆå•è½¨ï¼‰"""
        waveforms = []
        sample_rate = None

        for audio_dict in audio_list:
            waveform = audio_dict["waveform"]
            if len(waveform.shape) == 3:
                waveform = waveform[0]  # ç§»é™¤batchç»´åº¦

            waveforms.append(waveform * voice_volume)
            sample_rate = audio_dict["sample_rate"]

        # æ‹¼æ¥éŸ³é¢‘
        combined_waveform = torch.cat(waveforms, dim=1)

        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        torchaudio.save(temp_file.name, combined_waveform, sample_rate)
        temp_file.close()

        return temp_file.name

    def _get_timestamp(self):
        """è·å–æ—¶é—´æˆ³"""
        import time
        return str(int(time.time()))

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "EnhancedVideoComposer": EnhancedVideoComposer
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "EnhancedVideoComposer": "ğŸ¬ å¢å¼ºè§†é¢‘åˆæˆå™¨ï¼ˆ3è½¨éŸ³é¢‘ï¼‰"
}