"""
è§†é¢‘åˆæˆå™¨èŠ‚ç‚¹ - å®Œæ•´ç‰ˆ
åŸºäºåŸCozeå·¥ä½œæµè®¾è®¡ï¼Œé›†æˆåŠ¨ç”»æ•ˆæœå’Œè½¬åœºå¤„ç†
å°†éŸ³é¢‘åˆ—è¡¨å’Œå›¾ç‰‡åˆ—è¡¨åˆæˆä¸ºå…·æœ‰ä¸“ä¸šæ•ˆæœçš„MP4è§†é¢‘
"""

import os
import tempfile
import subprocess
import shutil
import torch
import torchaudio
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import cv2
import folder_paths
from typing import List, Dict, Any, Tuple, Generator
import json
import math

class VideoComposer:
    """
    è§†é¢‘åˆæˆå™¨ - åŸºç¡€éŸ³è§†é¢‘åŒæ­¥åˆæˆ

    å°†éŸ³é¢‘åˆ—è¡¨å’Œå›¾ç‰‡batchåˆæˆä¸ºæ—¶é—´åŒæ­¥çš„è§†é¢‘
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_list": ("*", {"tooltip": "éŸ³é¢‘åˆ—è¡¨ï¼Œæ¯ä¸ªéŸ³é¢‘å¯¹åº”ä¸€ä¸ªåœºæ™¯"}),
                "images": ("IMAGE", {"tooltip": "å›¾ç‰‡batchï¼Œæ¯å¼ å›¾ç‰‡å¯¹åº”ä¸€ä¸ªåœºæ™¯"}),
                "fps": ("INT", {
                    "default": 30,
                    "min": 15,
                    "max": 60,
                    "step": 1,
                    "tooltip": "è§†é¢‘å¸§ç‡"
                }),
                "width": ("INT", {
                    "default": 720,
                    "min": 480,
                    "max": 1920,
                    "step": 8,
                    "tooltip": "è§†é¢‘å®½åº¦"
                }),
                "height": ("INT", {
                    "default": 1280,
                    "min": 720,
                    "max": 2560,
                    "step": 8,
                    "tooltip": "è§†é¢‘é«˜åº¦"
                })
            },
            "optional": {
                "output_format": (["mp4", "avi", "mov"], {
                    "default": "mp4",
                    "tooltip": "è¾“å‡ºè§†é¢‘æ ¼å¼"
                }),
                "quality": (["high", "medium", "low"], {
                    "default": "medium",
                    "tooltip": "è§†é¢‘è´¨é‡"
                }),
                "animation_type": (["coze_zoom", "fade", "slide", "none"], {
                    "default": "coze_zoom",
                    "tooltip": "åŠ¨ç”»æ•ˆæœç±»å‹"
                }),
                "transition_duration": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "è½¬åœºæ—¶é•¿ï¼ˆç§’ï¼‰"
                }),
                # ä¸»è§’å›¾ç›¸å…³å‚æ•°
                "character_image": ("IMAGE", {
                    "tooltip": "ä¸»è§’å›¾ç‰‡ï¼Œç”¨äºé¦–å¸§ç‰¹æ•ˆï¼ˆå¯é€‰ï¼‰"
                }),
                "enable_character_intro": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨ä¸»è§’å¼€åœºåŠ¨ç”»"
                }),
                "char_intro_scale_start": ("FLOAT", {
                    "default": 2.0,
                    "min": 0.5,
                    "max": 5.0,
                    "step": 0.1,
                    "tooltip": "ä¸»è§’å›¾å¼€å§‹ç¼©æ”¾æ¯”ä¾‹"
                }),
                "char_intro_scale_mid": ("FLOAT", {
                    "default": 1.2,
                    "min": 0.5,
                    "max": 3.0,
                    "step": 0.1,
                    "tooltip": "ä¸»è§’å›¾ä¸­é—´ç¼©æ”¾æ¯”ä¾‹"
                }),
                "char_intro_scale_end": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.5,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "ä¸»è§’å›¾ç»“æŸç¼©æ”¾æ¯”ä¾‹"
                }),
                "char_intro_mid_timing": ("FLOAT", {
                    "default": 0.533,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "ä¸»è§’å›¾ä¸­é—´å…³é”®å¸§æ—¶é—´ç‚¹ï¼ˆç§’ï¼‰"
                }),
                # æ ‡é¢˜æ˜¾ç¤ºå‚æ•°
                "title_text": ("STRING", {
                    "default": "",
                    "tooltip": "è§†é¢‘æ ‡é¢˜æ–‡å­—ï¼ˆ2å­—ä¸»é¢˜ï¼‰"
                }),
                "enable_title": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨æ ‡é¢˜æ˜¾ç¤º"
                }),
                "title_duration": ("FLOAT", {
                    "default": 3.0,
                    "min": 1.0,
                    "max": 10.0,
                    "step": 0.5,
                    "tooltip": "æ ‡é¢˜æ˜¾ç¤ºæ—¶é•¿ï¼ˆç§’ï¼‰"
                }),
                "title_fontsize": ("INT", {
                    "default": 80,
                    "min": 30,
                    "max": 300,
                    "step": 5,
                    "tooltip": "æ ‡é¢˜å­—ä½“å¤§å°"
                }),
                "title_color": (["white", "black", "red", "gold", "blue"], {
                    "default": "white",
                    "tooltip": "æ ‡é¢˜é¢œè‰²"
                }),
                "title_font": (["è‡ªåŠ¨é€‰æ‹©", "Notoæ— è¡¬çº¿-å¸¸è§„", "Notoæ— è¡¬çº¿-ç²—ä½“", "Notoè¡¬çº¿ä½“", "æ–‡æ³‰é©¿æ­£é»‘", "Droidé»‘ä½“", "Arialé£æ ¼", "Helveticaé£æ ¼", "Timesé£æ ¼"], {
                    "default": "è‡ªåŠ¨é€‰æ‹©",
                    "tooltip": "æ ‡é¢˜å­—ä½“é€‰æ‹©ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"
                })
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("video_path", "info")
    FUNCTION = "compose_video"
    CATEGORY = "ğŸ”¥ Shenglin/è§†é¢‘å¤„ç†"
    DESCRIPTION = "å°†éŸ³é¢‘åˆ—è¡¨å’Œå›¾ç‰‡åˆæˆä¸ºåŒæ­¥è§†é¢‘"

    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()

    def compose_video(self, audio_list, images, fps=30, width=720, height=1280,
                     output_format="mp4", quality="medium", animation_type="coze_zoom",
                     transition_duration=0.5, character_image=None, enable_character_intro=True,
                     char_intro_scale_start=2.0, char_intro_scale_mid=1.2, char_intro_scale_end=1.0,
                     char_intro_mid_timing=0.533, title_text="", enable_title=False,
                     title_duration=3.0, title_fontsize=80, title_color="white", title_font="è‡ªåŠ¨é€‰æ‹©"):
        """
        åˆæˆè§†é¢‘çš„ä¸»å‡½æ•°
        """
        try:
            # æ£€æŸ¥è¾“å…¥
            if not isinstance(audio_list, list):
                raise ValueError("audio_listå¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹")

            if len(audio_list) == 0:
                raise ValueError("éŸ³é¢‘åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

            # æ£€æŸ¥éŸ³é¢‘å’Œå›¾åƒçš„å¯¹åº”å…³ç³»
            if enable_character_intro and character_image is not None:
                # æœ‰ä¸»è§’å›¾åƒæ—¶ï¼šéŸ³é¢‘æ•°é‡ = åœºæ™¯å›¾åƒæ•°é‡ + 1ï¼ˆä¸»è§’å›¾åƒå ç”¨ç¬¬ä¸€ä¸ªéŸ³é¢‘ï¼‰
                expected_audio_count = images.shape[0] + 1
                if len(audio_list) != expected_audio_count:
                    print(f"âš ï¸ è­¦å‘Šï¼šæœ‰ä¸»è§’å›¾åƒæ—¶ï¼ŒéŸ³é¢‘æ•°é‡({len(audio_list)}) åº”è¯¥ç­‰äº åœºæ™¯å›¾åƒæ•°é‡({images.shape[0]}) + 1")
                    if len(audio_list) < expected_audio_count:
                        print(f"âŒ éŸ³é¢‘æ•°é‡ä¸è¶³ï¼Œéœ€è¦ {expected_audio_count} ä¸ªéŸ³é¢‘")
                        raise ValueError(f"éŸ³é¢‘æ•°é‡ä¸è¶³ï¼šéœ€è¦{expected_audio_count}ä¸ªéŸ³é¢‘ï¼Œå®é™…{len(audio_list)}ä¸ª")
                    else:
                        # æˆªå–åˆ°æ­£ç¡®æ•°é‡
                        audio_list = audio_list[:expected_audio_count]
                        print(f"âœ‚ï¸ æˆªå–å‰{expected_audio_count}ä¸ªéŸ³é¢‘")
            else:
                # æ— ä¸»è§’å›¾åƒæ—¶ï¼šéŸ³é¢‘æ•°é‡ = åœºæ™¯å›¾åƒæ•°é‡
                if images.shape[0] != len(audio_list):
                    print(f"âš ï¸ è­¦å‘Šï¼šæ— ä¸»è§’å›¾åƒæ—¶ï¼Œå›¾ç‰‡æ•°é‡({images.shape[0]}) ä¸éŸ³é¢‘æ•°é‡({len(audio_list)}) ä¸åŒ¹é…")
                    # è°ƒæ•´åˆ°æœ€å°é•¿åº¦
                    min_count = min(images.shape[0], len(audio_list))
                    images = images[:min_count]
                    audio_list = audio_list[:min_count]

            print(f"ğŸ¬ å¼€å§‹è§†é¢‘åˆæˆï¼š{len(audio_list)} ä¸ªåœºæ™¯ï¼Œåˆ†è¾¨ç‡ {width}x{height}")

            # 1. åˆ†æéŸ³é¢‘æ—¶é•¿
            audio_durations = []
            total_duration = 0

            for i, audio_dict in enumerate(audio_list):
                if isinstance(audio_dict, dict) and "waveform" in audio_dict:
                    waveform = audio_dict["waveform"]
                    sample_rate = audio_dict["sample_rate"]

                    # è®¡ç®—æ—¶é•¿ï¼ˆç§’ï¼‰
                    if len(waveform.shape) == 3:
                        waveform = waveform[0]  # ç§»é™¤batchç»´åº¦

                    duration = waveform.shape[1] / sample_rate
                    audio_durations.append(duration)
                    total_duration += duration
                    print(f"ğŸµ åœºæ™¯ {i+1} éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")
                else:
                    raise ValueError(f"éŸ³é¢‘ {i} æ ¼å¼ä¸æ­£ç¡®ï¼Œéœ€è¦åŒ…å«waveformå’Œsample_rate")

            # 2. æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘
            print("ğŸ”Š æ‹¼æ¥éŸ³é¢‘...")
            combined_audio_path = self._combine_audio(audio_list)

            # 3. è®¡ç®—æ€»å¸§æ•°
            total_frames = sum(int(duration * fps) for duration in audio_durations)
            transition_frames_total = int(transition_duration * fps) * (len(images) - 1) if transition_duration > 0 else 0
            total_frames += transition_frames_total

            print(f"ğŸ“Š é¢„è®¡ç”Ÿæˆ {total_frames} å¸§ (å†…å­˜ä¼˜åŒ–æ¨¡å¼)")

            # 4. åˆæˆæœ€ç»ˆè§†é¢‘ï¼ˆä½¿ç”¨æµå¼å¤„ç†ï¼‰
            print("ğŸ¬ å¼€å§‹æµå¼è§†é¢‘åˆæˆ...")
            output_filename = f"story_video_{self._get_timestamp()}.{output_format}"
            video_path = os.path.join(self.output_dir, output_filename)

            # åˆ›å»ºå¸§ç”Ÿæˆå™¨ï¼ˆåŒ…å«ä¸»è§’å›¾å¤„ç†ï¼‰
            frame_generator = self._create_animated_frame_generator(
                images, audio_durations, fps, width, height,
                animation_type, transition_duration, character_image, enable_character_intro,
                char_intro_scale_start, char_intro_scale_mid, char_intro_scale_end, char_intro_mid_timing
            )

            # æ„å»ºæ ‡é¢˜é…ç½®
            title_config = {
                'enable_title': enable_title and title_text.strip(),
                'title_text': title_text.strip(),
                'fontsize': title_fontsize,
                'color': title_color,
                'duration': title_duration,
                'font': title_font
            } if enable_title and title_text.strip() else None

            self._merge_video_audio_streaming(
                frame_generator, combined_audio_path, video_path, fps, quality, total_frames, title_config
            )

            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(combined_audio_path)
            except:
                pass

            info = (f"è§†é¢‘åˆæˆå®Œæˆ\\n"
                   f"åœºæ™¯æ•°: {len(audio_list)}\\n"
                   f"æ€»æ—¶é•¿: {total_duration:.2f}ç§’\\n"
                   f"åˆ†è¾¨ç‡: {width}x{height}\\n"
                   f"å¸§ç‡: {fps}fps\\n"
                   f"è¾“å‡º: {output_filename}")

            print(f"âœ… è§†é¢‘åˆæˆå®Œæˆ: {video_path}")
            return (video_path, info)

        except Exception as e:
            error_msg = f"è§†é¢‘åˆæˆå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return ("", error_msg)

    def _combine_audio(self, audio_list):
        """æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘"""
        waveforms = []
        sample_rate = None

        for audio_dict in audio_list:
            waveform = audio_dict["waveform"]
            if len(waveform.shape) == 3:
                waveform = waveform[0]  # ç§»é™¤batchç»´åº¦

            waveforms.append(waveform)
            sample_rate = audio_dict["sample_rate"]

        # æ‹¼æ¥éŸ³é¢‘
        combined_waveform = torch.cat(waveforms, dim=1)

        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        torchaudio.save(temp_file.name, combined_waveform, sample_rate)
        temp_file.close()

        return temp_file.name

    def _create_animated_video_frames(self, images, durations, fps, width, height,
                                    animation_type, transition_duration):
        """
        åˆ›å»ºå¸¦åŠ¨ç”»æ•ˆæœçš„è§†é¢‘å¸§ - åŸºäºåŸCozeå·¥ä½œæµè®¾è®¡

        æ”¯æŒçš„åŠ¨ç”»ç±»å‹ï¼š
        - coze_zoom: åŸCozeå·¥ä½œæµçš„ç¼©æ”¾åŠ¨ç”»ï¼ˆå¥‡å¶äº¤æ›¿æ–¹å‘ï¼‰
        - fade: æ·¡å…¥æ·¡å‡ºæ•ˆæœ
        - slide: æ»‘åŠ¨æ•ˆæœ
        - none: æ— åŠ¨ç”»
        """
        all_frames = []
        transition_frames = int(transition_duration * fps) if transition_duration > 0 else 0

        for i, (image_tensor, duration) in enumerate(zip(images, durations)):
            # è½¬æ¢tensoråˆ°PILå›¾ç‰‡
            image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(image_np)

            # è°ƒæ•´å›¾ç‰‡å°ºå¯¸åˆ°ç•¥å¤§äºç›®æ ‡ï¼Œç”¨äºç¼©æ”¾åŠ¨ç”»
            if animation_type == "coze_zoom":
                # Cozeé£æ ¼ï¼šå›¾ç‰‡ç¨å¤§ä¸€äº›ï¼Œç”¨äºç¼©æ”¾
                scale_factor = 1.3
                scaled_width = int(width * scale_factor)
                scaled_height = int(height * scale_factor)
                pil_image = pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
            else:
                pil_image = pil_image.resize((width, height), Image.Resampling.LANCZOS)

            # è®¡ç®—éœ€è¦çš„å¸§æ•°
            total_frames = int(duration * fps)
            content_frames = total_frames - (transition_frames if i < len(images) - 1 else 0)

            print(f"ğŸ“¹ åœºæ™¯ {i+1}: ç”Ÿæˆ {total_frames} å¸§ ({duration:.2f}ç§’)")
            print(f"   å†…å®¹å¸§: {content_frames}, è½¬åœºå¸§: {transition_frames if i < len(images) - 1 else 0}")

            # ç”ŸæˆåŠ¨ç”»å¸§
            scene_frames = self._generate_scene_animation_frames(
                pil_image, content_frames, width, height, animation_type, i
            )
            all_frames.extend(scene_frames)

            # æ·»åŠ è½¬åœºæ•ˆæœï¼ˆé™¤äº†æœ€åä¸€ä¸ªåœºæ™¯ï¼‰
            if i < len(images) - 1 and transition_frames > 0:
                next_image_tensor = images[i + 1]
                next_image_np = (next_image_tensor.cpu().numpy() * 255).astype(np.uint8)
                next_pil_image = Image.fromarray(next_image_np)

                if animation_type == "coze_zoom":
                    next_pil_image = next_pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
                else:
                    next_pil_image = next_pil_image.resize((width, height), Image.Resampling.LANCZOS)

                transition_frames_list = self._generate_transition_frames(
                    pil_image, next_pil_image, transition_frames, width, height, animation_type
                )
                all_frames.extend(transition_frames_list)

        return all_frames

    def _create_animated_frame_generator(self, images, durations, fps, width, height,
                                       animation_type, transition_duration, character_image=None,
                                       enable_character_intro=False, char_intro_scale_start=2.0,
                                       char_intro_scale_mid=1.2, char_intro_scale_end=1.0,
                                       char_intro_mid_timing=0.533) -> Generator[np.ndarray, None, None]:
        """
        åˆ›å»ºå¸¦åŠ¨ç”»æ•ˆæœçš„å¸§ç”Ÿæˆå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        ä½¿ç”¨Generatoræ¨¡å¼ï¼Œæ¯æ¬¡åªç”Ÿæˆä¸€å¸§ï¼Œé¿å…å†…å­˜æº¢å‡º
        """
        transition_frames = int(transition_duration * fps) if transition_duration > 0 else 0

        # å¤„ç†ä¸»è§’å›¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        character_pil = None
        print(f"ğŸ‘¤ ä¸»è§’å›¾åƒçŠ¶æ€æ£€æŸ¥:")
        print(f"   - enable_character_intro: {enable_character_intro}")
        print(f"   - character_image is not None: {character_image is not None}")

        if enable_character_intro and character_image is not None:
            print("âœ… å¯ç”¨ä¸»è§’å›¾åƒï¼Œå‡†å¤‡ä¸»è§’å¼€åœºåŠ¨ç”»...")
            char_np = (character_image[0].cpu().numpy() * 255).astype(np.uint8)
            character_pil = Image.fromarray(char_np)
            print(f"   - åŸå§‹ä¸»è§’å›¾å°ºå¯¸: {character_pil.size}")
            # ä¸»è§’å›¾è°ƒæ•´åˆ°åˆé€‚å°ºå¯¸
            if animation_type == "coze_zoom":
                scale_factor = 1.3
                scaled_width = int(width * scale_factor)
                scaled_height = int(height * scale_factor)
                character_pil = character_pil.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
                print(f"   - è°ƒæ•´åä¸»è§’å›¾å°ºå¯¸: {character_pil.size} (ç¼©æ”¾å› å­: {scale_factor})")
            else:
                character_pil = character_pil.resize((width, height), Image.Resampling.LANCZOS)
                print(f"   - è°ƒæ•´åä¸»è§’å›¾å°ºå¯¸: {character_pil.size}")
        elif enable_character_intro and character_image is None:
            print("âš ï¸ ä¸»è§’å¼€åœºåŠ¨ç”»å·²å¯ç”¨ï¼Œä½†æœªæä¾›ä¸»è§’å›¾åƒ")
        elif not enable_character_intro and character_image is not None:
            print("âš ï¸ å·²æä¾›ä¸»è§’å›¾åƒï¼Œä½†ä¸»è§’å¼€åœºåŠ¨ç”»æœªå¯ç”¨")
        else:
            print("â„¹ï¸ æœªå¯ç”¨ä¸»è§’å¼€åœºåŠ¨ç”»")

        # é‡æ„éŸ³é¢‘-å›¾åƒå¯¹åº”é€»è¾‘
        current_duration_index = 0

        # 1. å¤„ç†ä¸»è§’å›¾å¼€åœºåŠ¨ç”»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if enable_character_intro and character_pil is not None:
            print("ğŸ­ æ·»åŠ ä¸»è§’å¼€åœºåŠ¨ç”»...")
            character_duration = durations[current_duration_index]
            print(f"   - ä¸»è§’å›¾ä½¿ç”¨éŸ³é¢‘[{current_duration_index}], æ—¶é•¿: {character_duration:.2f}ç§’")

            # ç”Ÿæˆä¸»è§’å¼€åœºåŠ¨ç”»å¸§
            for frame in self._generate_character_intro_frames(
                character_pil, character_duration, fps, width, height,
                char_intro_scale_start, char_intro_scale_mid, char_intro_scale_end, char_intro_mid_timing
            ):
                yield frame

            current_duration_index += 1  # ä¸»è§’å›¾å ç”¨äº†ç¬¬ä¸€ä¸ªéŸ³é¢‘

        # 2. å¤„ç†æ‰€æœ‰åœºæ™¯å›¾åƒ
        for i, image_tensor in enumerate(images):
            if current_duration_index >= len(durations):
                print(f"âš ï¸ è­¦å‘Šï¼šåœºæ™¯{i+1}æ²¡æœ‰å¯¹åº”çš„éŸ³é¢‘ï¼Œè·³è¿‡")
                break

            duration = durations[current_duration_index]
            print(f"ğŸ“¹ å¤„ç†åœºæ™¯ {i+1}/{len(images)}, ä½¿ç”¨éŸ³é¢‘[{current_duration_index}], æ—¶é•¿: {duration:.2f}ç§’")

            # è½¬æ¢tensoråˆ°PILå›¾ç‰‡
            image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(image_np)

            # è°ƒæ•´å›¾ç‰‡å°ºå¯¸åˆ°ç•¥å¤§äºç›®æ ‡ï¼Œç”¨äºç¼©æ”¾åŠ¨ç”»
            if animation_type == "coze_zoom":
                scale_factor = 1.3
                scaled_width = int(width * scale_factor)
                scaled_height = int(height * scale_factor)
                pil_image = pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
            else:
                pil_image = pil_image.resize((width, height), Image.Resampling.LANCZOS)

            # è®¡ç®—éœ€è¦çš„å¸§æ•°
            total_frames = int(duration * fps)
            content_frames = total_frames - (transition_frames if i < len(images) - 1 else 0)

            # ç”Ÿæˆåœºæ™¯åŠ¨ç”»å¸§
            for frame in self._generate_scene_animation_frames_generator(
                pil_image, content_frames, width, height, animation_type, i
            ):
                yield frame

            # ç”Ÿæˆè½¬åœºå¸§ï¼ˆé™¤äº†æœ€åä¸€ä¸ªåœºæ™¯ï¼‰
            if i < len(images) - 1 and transition_frames > 0:
                next_image_tensor = images[i + 1]
                next_image_np = (next_image_tensor.cpu().numpy() * 255).astype(np.uint8)
                next_pil_image = Image.fromarray(next_image_np)

                if animation_type == "coze_zoom":
                    next_pil_image = next_pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
                else:
                    next_pil_image = next_pil_image.resize((width, height), Image.Resampling.LANCZOS)

                # è®¡ç®—è½¬åœºæ—¶çš„ç¼©æ”¾è¿ç»­æ€§
                current_scene_is_odd = i % 2 == 0
                next_scene_is_odd = (i + 1) % 2 == 0

                # æ­£ç¡®çš„ç¼©æ”¾è¿ç»­æ€§ï¼šå½“å‰åœºæ™¯ç»“æŸå€¼ = ä¸‹ä¸€åœºæ™¯å¼€å§‹å€¼
                if current_scene_is_odd:
                    # å¥‡æ•°åœºæ™¯: 1.0â†’1.5ï¼Œç»“æŸå€¼æ˜¯1.5
                    current_end_scale = 1.5
                    next_start_scale = 1.5  # ä¸‹ä¸€åœºæ™¯ä»1.5å¼€å§‹
                else:
                    # å¶æ•°åœºæ™¯: 1.5â†’1.0ï¼Œç»“æŸå€¼æ˜¯1.0
                    current_end_scale = 1.0
                    next_start_scale = 1.0  # ä¸‹ä¸€åœºæ™¯ä»1.0å¼€å§‹

                print(f"ğŸ”„ è½¬åœº {i+1}â†’{i+2}: {current_end_scale:.1f} â†’ {next_start_scale:.1f} (è¿ç»­ç¼©æ”¾)")

                for frame in self._generate_transition_frames_generator(
                    pil_image, next_pil_image, transition_frames, width, height,
                    animation_type, current_end_scale, next_start_scale
                ):
                    yield frame

            # é€’å¢éŸ³é¢‘ç´¢å¼•ï¼Œå‡†å¤‡å¤„ç†ä¸‹ä¸€ä¸ªåœºæ™¯
            current_duration_index += 1

    def _generate_character_intro_frames(self, character_pil, duration, fps, width, height,
                                       scale_start, scale_mid, scale_end, mid_timing) -> Generator[np.ndarray, None, None]:
        """
        ç”Ÿæˆä¸»è§’å¼€åœºåŠ¨ç”»å¸§
        å¤åˆ»åŸCozeå·¥ä½œæµçš„3æ®µå¼ç¼©æ”¾åŠ¨ç”»ï¼š2.0â†’1.2â†’1.0
        """
        total_frames = int(duration * fps)
        mid_frame = int(mid_timing * fps)  # ä¸­é—´å…³é”®å¸§ä½ç½®

        print(f"ğŸ‘¤ ä¸»è§’åŠ¨ç”»: {scale_start:.1f}â†’{scale_mid:.1f}â†’{scale_end:.1f} (æ€»å¸§æ•°:{total_frames}, ä¸­é—´å¸§:{mid_frame})")

        for frame_idx in range(total_frames):
            if frame_idx <= mid_frame:
                # ç¬¬ä¸€æ®µï¼šstart â†’ mid
                progress = frame_idx / max(mid_frame, 1)
                eased_progress = self._ease_in_out(progress)
                current_scale = scale_start + (scale_mid - scale_start) * eased_progress
            else:
                # ç¬¬äºŒæ®µï¼šmid â†’ end
                progress = (frame_idx - mid_frame) / max(total_frames - mid_frame, 1)
                eased_progress = self._ease_in_out(progress)
                current_scale = scale_mid + (scale_end - scale_mid) * eased_progress

            # æ¯100å¸§è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            if frame_idx % 100 == 0:
                print(f"     ä¸»è§’å¸§{frame_idx}: scale={current_scale:.3f}")

            # åº”ç”¨ç¼©æ”¾æ•ˆæœ
            frame = self._apply_zoom_effect(character_pil, current_scale, width, height)
            yield frame

    def _generate_scene_animation_frames_generator(self, pil_image, frame_count, width, height,
                                                 animation_type, scene_index) -> Generator[np.ndarray, None, None]:
        """åœºæ™¯åŠ¨ç”»å¸§ç”Ÿæˆå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""

        if animation_type == "coze_zoom":
            # Cozeé£æ ¼ç¼©æ”¾åŠ¨ç”»
            is_odd_scene = scene_index % 2 == 0

            if is_odd_scene:
                start_scale, end_scale = 1.0, 1.5
            else:
                start_scale, end_scale = 1.5, 1.0

            for frame_idx in range(frame_count):
                progress = frame_idx / max(frame_count - 1, 1)
                eased_progress = self._ease_in_out(progress)
                current_scale = start_scale + (end_scale - start_scale) * eased_progress

                # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100å¸§è¾“å‡ºä¸€æ¬¡å½“å‰ç¼©æ”¾å€¼
                if frame_idx % 100 == 0:
                    print(f"     å¸§{frame_idx}: scale={current_scale:.3f} (progress={progress:.3f})")

                frame = self._apply_zoom_effect(pil_image, current_scale, width, height)
                yield frame

        elif animation_type == "fade":
            # æ·¡å…¥æ•ˆæœ
            base_frame = np.array(pil_image.resize((width, height), Image.Resampling.LANCZOS))
            base_frame_bgr = cv2.cvtColor(base_frame, cv2.COLOR_RGB2BGR)

            for frame_idx in range(frame_count):
                progress = min(frame_idx / (frame_count * 0.2), 1.0)
                alpha = progress
                frame = base_frame_bgr * alpha
                yield frame.astype(np.uint8)

        else:  # none æˆ–å…¶ä»–
            # é™æ€å¸§
            base_frame = np.array(pil_image.resize((width, height), Image.Resampling.LANCZOS))
            base_frame_bgr = cv2.cvtColor(base_frame, cv2.COLOR_RGB2BGR)

            for _ in range(frame_count):
                yield base_frame_bgr

    def _generate_transition_frames_generator(self, current_image, next_image, frame_count,
                                            width, height, animation_type,
                                            current_end_scale=1.0, next_start_scale=1.0) -> Generator[np.ndarray, None, None]:
        """è½¬åœºå¸§ç”Ÿæˆå™¨ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒç¼©æ”¾è¿ç»­æ€§ï¼‰"""

        if animation_type == "coze_zoom":
            # Cozeç¼©æ”¾è½¬åœºï¼šä¿æŒç¼©æ”¾è¿ç»­æ€§çš„äº¤å‰æ·¡åŒ–
            for frame_idx in range(frame_count):
                progress = frame_idx / max(frame_count - 1, 1)
                alpha = self._ease_in_out(progress)

                # ä¿æŒè¿ç»­ç¼©æ”¾ï¼šä¸¤ä¸ªå›¾åƒéƒ½ä½¿ç”¨ç›¸åŒçš„ç¼©æ”¾å€¼
                # è¿™æ ·å°±ä¸ä¼šæœ‰ç¼©æ”¾è·³è·ƒï¼Œåªæœ‰å›¾åƒå†…å®¹çš„æ·¡å…¥æ·¡å‡º
                scale_value = current_end_scale  # == next_start_scaleï¼Œä¿æŒè¿ç»­

                current_frame = self._apply_zoom_effect(current_image, scale_value, width, height)
                next_frame = self._apply_zoom_effect(next_image, scale_value, width, height)

                # äº¤å‰æ·¡åŒ–
                current_frame_float = current_frame.astype(np.float32)
                next_frame_float = next_frame.astype(np.float32)

                blended = current_frame_float * (1 - alpha) + next_frame_float * alpha
                yield blended.astype(np.uint8)

        elif animation_type == "fade":
            # ç®€å•æ·¡åŒ–è½¬åœº
            current_frame = np.array(current_image.resize((width, height), Image.Resampling.LANCZOS))
            next_frame = np.array(next_image.resize((width, height), Image.Resampling.LANCZOS))

            current_bgr = cv2.cvtColor(current_frame, cv2.COLOR_RGB2BGR)
            next_bgr = cv2.cvtColor(next_frame, cv2.COLOR_RGB2BGR)

            for frame_idx in range(frame_count):
                progress = frame_idx / max(frame_count - 1, 1)
                alpha = progress
                blended = current_bgr * (1 - alpha) + next_bgr * alpha
                yield blended.astype(np.uint8)

        else:
            # ç¡¬åˆ‡æ¢è½¬åœº
            current_frame = np.array(current_image.resize((width, height), Image.Resampling.LANCZOS))
            next_frame = np.array(next_image.resize((width, height), Image.Resampling.LANCZOS))

            current_bgr = cv2.cvtColor(current_frame, cv2.COLOR_RGB2BGR)
            next_bgr = cv2.cvtColor(next_frame, cv2.COLOR_RGB2BGR)

            for frame_idx in range(frame_count):
                progress = frame_idx / max(frame_count - 1, 1)
                if progress < 0.5:
                    yield current_bgr
                else:
                    yield next_bgr

    def _generate_scene_animation_frames(self, pil_image, frame_count, width, height,
                                       animation_type, scene_index):
        """
        ä¸ºå•ä¸ªåœºæ™¯ç”ŸæˆåŠ¨ç”»å¸§

        Cozeå·¥ä½œæµçš„æ ¸å¿ƒåŠ¨ç”»ï¼š
        - å¥‡æ•°åœºæ™¯ï¼šä»1.0ç¼©æ”¾åˆ°1.5
        - å¶æ•°åœºæ™¯ï¼šä»1.5ç¼©æ”¾åˆ°1.0
        """
        frames = []

        if animation_type == "coze_zoom":
            # Cozeé£æ ¼ç¼©æ”¾åŠ¨ç”»
            is_odd_scene = scene_index % 2 == 0  # 0-indexedï¼Œæ‰€ä»¥å¶æ•°ç´¢å¼•æ˜¯å¥‡æ•°åœºæ™¯

            if is_odd_scene:
                # å¥‡æ•°åœºæ™¯ï¼š1.0 â†’ 1.5ï¼ˆæ”¾å¤§ï¼‰
                start_scale = 1.0
                end_scale = 1.5
            else:
                # å¶æ•°åœºæ™¯ï¼š1.5 â†’ 1.0ï¼ˆç¼©å°ï¼‰
                start_scale = 1.5
                end_scale = 1.0

            print(f"   Cozeç¼©æ”¾: {start_scale:.1f} â†’ {end_scale:.1f}")

            for frame_idx in range(frame_count):
                # è®¡ç®—å½“å‰å¸§çš„ç¼©æ”¾æ¯”ä¾‹ï¼ˆä½¿ç”¨ç¼“åŠ¨å‡½æ•°ï¼‰
                progress = frame_idx / max(frame_count - 1, 1)
                # ä½¿ç”¨ease-in-outç¼“åŠ¨
                eased_progress = self._ease_in_out(progress)
                current_scale = start_scale + (end_scale - start_scale) * eased_progress

                # åº”ç”¨ç¼©æ”¾æ•ˆæœ
                frame = self._apply_zoom_effect(pil_image, current_scale, width, height)
                frames.append(frame)

        elif animation_type == "fade":
            # æ·¡å…¥æ•ˆæœ
            base_frame = np.array(pil_image.resize((width, height), Image.Resampling.LANCZOS))
            base_frame_bgr = cv2.cvtColor(base_frame, cv2.COLOR_RGB2BGR)

            for frame_idx in range(frame_count):
                progress = min(frame_idx / (frame_count * 0.2), 1.0)  # å‰20%æ—¶é—´æ·¡å…¥
                alpha = progress
                frame = base_frame_bgr * alpha
                frames.append(frame.astype(np.uint8))

        else:  # none æˆ–å…¶ä»–
            # é™æ€å¸§
            base_frame = np.array(pil_image.resize((width, height), Image.Resampling.LANCZOS))
            base_frame_bgr = cv2.cvtColor(base_frame, cv2.COLOR_RGB2BGR)

            for _ in range(frame_count):
                frames.append(base_frame_bgr)

        return frames

    def _apply_zoom_effect(self, pil_image, scale, target_width, target_height):
        """
        åº”ç”¨Ken Burnsç¼©æ”¾æ•ˆæœï¼ˆå®Œå…¨é‡æ„ç‰ˆ - åŸºäºå›¾åƒå˜æ¢æ¶ˆé™¤æ™ƒåŠ¨ï¼‰

        æ–°ç®—æ³•ï¼š
        1. å…ˆå°†åŸå›¾è°ƒæ•´åˆ°é€‚å½“å¤§å°
        2. ç„¶åå±…ä¸­è£å‰ªåˆ°ç›®æ ‡å°ºå¯¸
        3. ç¡®ä¿ç¼©æ”¾ä¸­å¿ƒå§‹ç»ˆç¨³å®š

        scale = 1.0: æ˜¾ç¤ºå®Œæ•´å›¾ç‰‡
        scale = 1.5: æ”¾å¤§1.5å€ï¼ˆæ˜¾ç¤ºä¸­å¿ƒ2/3åŒºåŸŸï¼‰
        scale = 2.0: æ”¾å¤§2å€ï¼ˆæ˜¾ç¤ºä¸­å¿ƒ1/2åŒºåŸŸï¼‰
        """
        img_width, img_height = pil_image.size

        # è®¡ç®—ç›®æ ‡å›¾åƒçš„å®é™…æ˜¾ç¤ºå°ºå¯¸ï¼ˆåº”ç”¨ç¼©æ”¾å› å­ï¼‰
        # scale > 1.0 è¡¨ç¤ºæ”¾å¤§ï¼Œéœ€è¦æ›´å¤§çš„ä¸­é—´å›¾åƒ
        intermediate_width = int(target_width * scale)
        intermediate_height = int(target_height * scale)

        # æ­¥éª¤1ï¼šå°†åŸå›¾ç¼©æ”¾åˆ°ä¸­é—´å°ºå¯¸ï¼Œä¿æŒçºµæ¨ªæ¯”
        # æ‰¾åˆ°åˆé€‚çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿èƒ½å®Œå…¨è¦†ç›–ä¸­é—´å°ºå¯¸
        scale_x = intermediate_width / img_width
        scale_y = intermediate_height / img_height
        uniform_scale = max(scale_x, scale_y)  # é€‰æ‹©è¾ƒå¤§çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–

        # åº”ç”¨ç­‰æ¯”ä¾‹ç¼©æ”¾ï¼ˆä½¿ç”¨é«˜ç²¾åº¦è®¡ç®—é¿å…æ™ƒåŠ¨ï¼‰
        # ä½¿ç”¨æµ®ç‚¹æ•°ç²¾ç¡®è®¡ç®—ï¼Œç„¶åå››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„å¶æ•°åƒç´ 
        scaled_width_float = img_width * uniform_scale
        scaled_height_float = img_height * uniform_scale

        # ä½¿ç”¨æ›´ç¨³å®šçš„å–æ•´æ–¹æ³•
        scaled_width = int(round(scaled_width_float))
        scaled_height = int(round(scaled_height_float))

        # ç¡®ä¿å°ºå¯¸ä¸ºå¶æ•°ï¼ˆè§†é¢‘ç¼–ç å‹å¥½ï¼Œé¿å…äºšåƒç´ é—®é¢˜ï¼‰
        if scaled_width % 2 == 1:
            scaled_width += 1
        if scaled_height % 2 == 1:
            scaled_height += 1

        # ä½¿ç”¨é«˜è´¨é‡çš„é‡é‡‡æ ·ç®—æ³•
        scaled_image = pil_image.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)

        # æ­¥éª¤2ï¼šä»ç¼©æ”¾åçš„å›¾åƒä¸­å¿ƒè£å‰ªç›®æ ‡å°ºå¯¸ï¼ˆç²¾ç¡®å±…ä¸­ï¼‰
        # ä½¿ç”¨æµ®ç‚¹æ•°ç²¾ç¡®è®¡ç®—ä¸­å¿ƒä½ç½®ï¼Œé¿å…æ•´æ•°é™¤æ³•è¯¯å·®
        center_x_float = scaled_width / 2.0
        center_y_float = scaled_height / 2.0

        half_target_width_float = target_width / 2.0
        half_target_height_float = target_height / 2.0

        # ç²¾ç¡®è®¡ç®—è£å‰ªåŒºåŸŸ
        left_float = center_x_float - half_target_width_float
        top_float = center_y_float - half_target_height_float

        # å››èˆäº”å…¥åˆ°æ•´æ•°åƒç´ 
        left = int(round(left_float))
        top = int(round(top_float))
        right = left + target_width
        bottom = top + target_height

        # è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿è£å‰ªåŒºåŸŸåœ¨å›¾åƒèŒƒå›´å†…
        left = max(0, left)
        top = max(0, top)
        right = min(scaled_width, right)
        bottom = min(scaled_height, bottom)

        # é‡æ–°è®¡ç®—å®é™…çš„è£å‰ªå°ºå¯¸
        actual_width = right - left
        actual_height = bottom - top

        # æ‰§è¡Œè£å‰ª
        if actual_width > 0 and actual_height > 0:
            cropped = scaled_image.crop((left, top, right, bottom))

            # å¦‚æœè£å‰ªåçš„å°ºå¯¸ä¸ç­‰äºç›®æ ‡å°ºå¯¸ï¼Œè¿›è¡Œæœ€ç»ˆè°ƒæ•´
            if cropped.size != (target_width, target_height):
                cropped = cropped.resize((target_width, target_height), Image.Resampling.LANCZOS)
        else:
            # å¼‚å¸¸æƒ…å†µï¼Œä½¿ç”¨åŸå›¾ç›´æ¥ç¼©æ”¾
            cropped = pil_image.resize((target_width, target_height), Image.Resampling.LANCZOS)

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        frame_np = np.array(cropped)
        frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)

        return frame_bgr

    def _generate_transition_frames(self, current_image, next_image, frame_count,
                                  width, height, animation_type):
        """ç”Ÿæˆè½¬åœºå¸§"""
        frames = []

        current_frame = np.array(current_image.resize((width, height), Image.Resampling.LANCZOS))
        next_frame = np.array(next_image.resize((width, height), Image.Resampling.LANCZOS))

        current_bgr = cv2.cvtColor(current_frame, cv2.COLOR_RGB2BGR)
        next_bgr = cv2.cvtColor(next_frame, cv2.COLOR_RGB2BGR)

        for frame_idx in range(frame_count):
            progress = frame_idx / max(frame_count - 1, 1)

            if animation_type == "fade":
                # äº¤å‰æ·¡åŒ–
                alpha = progress
                blended = current_bgr * (1 - alpha) + next_bgr * alpha
                frames.append(blended.astype(np.uint8))
            else:
                # é»˜è®¤ï¼šå¿«é€Ÿåˆ‡æ¢ï¼ˆç”¨äºå…¶ä»–åŠ¨ç”»ç±»å‹ï¼‰
                if progress < 0.5:
                    frames.append(current_bgr)
                else:
                    frames.append(next_bgr)

        return frames

    def _ease_in_out(self, t):
        """ç¼“åŠ¨å‡½æ•°ï¼šçº¿æ€§æ’å€¼ï¼ˆä¿®å¤æ™ƒåŠ¨é—®é¢˜ï¼‰"""
        # åŸæ¥çš„ ease-in-out: t * t * (3.0 - 2.0 * t)
        # æ”¹ä¸ºçº¿æ€§æ’å€¼ï¼Œæ¶ˆé™¤åŠ é€Ÿåº¦å˜åŒ–å¯¼è‡´çš„è§†è§‰æ™ƒåŠ¨
        return t  # çº¿æ€§æ’å€¼ï¼Œæ— åŠ é€Ÿåº¦å˜åŒ–

    def _merge_video_audio_streaming(self, frame_generator, audio_path, output_path, fps, quality, total_frames, title_config=None):
        """ä½¿ç”¨æµå¼å¤„ç†åˆæˆè§†é¢‘ï¼Œé¿å…å†…å­˜æº¢å‡º"""
        temp_video_path = output_path + ".temp.mp4"

        try:
            # è®¾ç½®è§†é¢‘ç¼–ç å™¨å‚æ•°
            if quality == "high":
                crf = 18
            elif quality == "medium":
                crf = 23
            else:  # low
                crf = 28

            print(f"ğŸ¬ å¼€å§‹æµå¼å†™å…¥è§†é¢‘: {total_frames} å¸§")

            # è·å–ç¬¬ä¸€å¸§ç¡®å®šè§†é¢‘å°ºå¯¸
            first_frame = next(frame_generator)
            height, width = first_frame.shape[:2]

            # ä½¿ç”¨OpenCVåˆ›å»ºè§†é¢‘å†™å…¥å™¨
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))

            if not out.isOpened():
                raise Exception("æ— æ³•åˆ›å»ºè§†é¢‘å†™å…¥å™¨")

            # å†™å…¥ç¬¬ä¸€å¸§
            out.write(first_frame)
            frames_written = 1

            # æµå¼å†™å…¥å‰©ä½™å¸§
            for frame in frame_generator:
                out.write(frame)
                frames_written += 1

                # æ¯1000å¸§æŠ¥å‘Šè¿›åº¦
                if frames_written % 1000 == 0:
                    progress = (frames_written / total_frames) * 100
                    print(f"ğŸ“¹ å†™å…¥è¿›åº¦: {frames_written}/{total_frames} ({progress:.1f}%)")

            out.release()
            print(f"âœ… è§†é¢‘å†™å…¥å®Œæˆ: {frames_written} å¸§")

            # ä½¿ç”¨ffmpegæ·»åŠ éŸ³é¢‘
            self._add_audio_with_ffmpeg(temp_video_path, audio_path, output_path, crf, title_config)

        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_video_path):
                os.unlink(temp_video_path)
            raise e

    def _add_audio_with_ffmpeg(self, video_path, audio_path, output_path, crf, title_config=None):
        """ä½¿ç”¨ffmpegæ·»åŠ éŸ³é¢‘å’Œå¯é€‰çš„æ ‡é¢˜æ–‡å­—"""
        try:
            # æ„å»ºffmpegå‘½ä»¤
            cmd = ['ffmpeg', '-y', '-i', video_path, '-i', audio_path]

            # æ·»åŠ æ–‡å­—æ»¤é•œï¼ˆå¦‚æœå¯ç”¨æ ‡é¢˜ï¼‰
            if title_config and title_config.get('enable_title') and title_config.get('title_text'):
                title_text = title_config['title_text']
                fontsize = title_config.get('fontsize', 80)
                color = title_config.get('color', 'white')
                duration = title_config.get('duration', 3.0)

                # è½¬ä¹‰æ–‡å­—ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                title_text = title_text.replace(":", "\\:")
                title_text = title_text.replace("'", "\\'")

                # è·å–æŒ‡å®šå­—ä½“
                font_name = title_config.get('font', 'auto')
                font_path = self._get_font_path(font_name)

                # æ„å»ºdrawtextæ»¤é•œ
                # x=(w-text_w)/2 æ°´å¹³å±…ä¸­, y=(h-text_h)/2 å‚ç›´å±…ä¸­
                # enable='between(t,0,{duration})' åªåœ¨å¼€å¤´Nç§’æ˜¾ç¤º
                if font_path:
                    text_filter = f"drawtext=text='{title_text}':fontfile='{font_path}':fontsize={fontsize}:fontcolor={color}:x=(w-text_w)/2:y=(h-text_h)/2:enable='between(t,0,{duration})'"
                else:
                    # å›é€€åˆ°é»˜è®¤å­—ä½“ï¼ˆå¯èƒ½æ— æ³•æ˜¾ç¤ºä¸­æ–‡ï¼‰
                    text_filter = f"drawtext=text='{title_text}':fontsize={fontsize}:fontcolor={color}:x=(w-text_w)/2:y=(h-text_h)/2:enable='between(t,0,{duration})'"
                    print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“")

                cmd.extend(['-vf', text_filter])
                print(f"ğŸ“ æ·»åŠ æ ‡é¢˜: '{title_text}' (æ˜¾ç¤º{duration}ç§’)")

            # è§†é¢‘å’ŒéŸ³é¢‘ç¼–ç å‚æ•°
            cmd.extend([
                '-c:v', 'libx264',
                '-crf', str(crf),
                '-preset', 'medium',
                '-c:a', 'aac',
                '-b:a', '128k',
                '-shortest',  # ä»¥æœ€çŸ­çš„æµä¸ºå‡†
                output_path
            ])

            print("ğŸ”Š æ·»åŠ éŸ³é¢‘è½¨é“...")
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)

            # æ¸…ç†ä¸´æ—¶è§†é¢‘æ–‡ä»¶
            os.unlink(video_path)
            print("ğŸµ éŸ³é¢‘åˆå¹¶æˆåŠŸ")

        except subprocess.CalledProcessError as e:
            print(f"âš ï¸ ffmpegæ‰§è¡Œå¤±è´¥: {e.stderr}")
            # å¦‚æœffmpegå¤±è´¥ï¼Œä½¿ç”¨åŸè§†é¢‘
            shutil.move(video_path, output_path)
            print("ğŸ“¹ ä½¿ç”¨æ— éŸ³é¢‘è§†é¢‘")

        except FileNotFoundError:
            print("âš ï¸ ffmpegæœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ— éŸ³é¢‘è§†é¢‘")
            shutil.move(video_path, output_path)

    def _get_font_path(self, font_name):
        """æ ¹æ®å­—ä½“åç§°è·å–å­—ä½“è·¯å¾„"""
        import platform

        # å­—ä½“æ˜ å°„è¡¨
        font_mapping = self._get_font_mapping()

        # ä¸­æ–‡é€‰é¡¹æ˜ å°„åˆ°è‹±æ–‡key
        font_name_mapping = {
            "è‡ªåŠ¨é€‰æ‹©": "auto",
            "Notoæ— è¡¬çº¿-å¸¸è§„": "noto_cjk_regular",
            "Notoæ— è¡¬çº¿-ç²—ä½“": "noto_cjk_bold",
            "Notoè¡¬çº¿ä½“-ç²—ä½“": "noto_serif_cjk_bold",
            "æ–‡æ³‰é©¿æ­£é»‘": "wqy_zenhei",
            "è¶…çº§ç²—ä½“": "nimbus_sans_bold",
            "è‹±æ–‡ç²—ä½“": "noto_serif_bold",
            "Droidé»‘ä½“": "droid_sans",
            "Arialé£æ ¼": "arial",
            "Helveticaé£æ ¼": "helvetica",
            "Timesé£æ ¼": "times"
        }

        # å¦‚æœæ˜¯ä¸­æ–‡é€‰é¡¹ï¼Œè½¬æ¢ä¸ºè‹±æ–‡key
        if font_name in font_name_mapping:
            font_name = font_name_mapping[font_name]

        if font_name == "auto":
            # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å­—ä½“
            return self._find_best_font()

        if font_name in font_mapping:
            # æ£€æŸ¥æŒ‡å®šå­—ä½“æ˜¯å¦å­˜åœ¨
            font_paths = font_mapping[font_name]
            for font_path in font_paths:
                if os.path.exists(font_path):
                    print(f"ğŸ”¤ ä½¿ç”¨æŒ‡å®šå­—ä½“: {font_name} -> {font_path}")
                    return font_path

            print(f"âš ï¸ æŒ‡å®šå­—ä½“ {font_name} ä¸å­˜åœ¨ï¼Œå›é€€åˆ°è‡ªåŠ¨é€‰æ‹©")
            return self._find_best_font()

        print(f"âš ï¸ æœªçŸ¥å­—ä½“åç§°: {font_name}ï¼Œå›é€€åˆ°è‡ªåŠ¨é€‰æ‹©")
        return self._find_best_font()

    def _get_font_mapping(self):
        """è·å–å­—ä½“åç§°åˆ°è·¯å¾„çš„æ˜ å°„"""
        # è·å–å†…ç½®å­—ä½“åŒ…è·¯å¾„
        bundled_fonts_dir = self._get_bundled_fonts_dir()

        # å†…ç½®å­—ä½“åŒ…æ˜ å°„ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
        bundled_fonts = {
            "noto_cjk_regular": os.path.join(bundled_fonts_dir, "NotoSansCJK-Regular.ttc"),
            "noto_cjk_bold": os.path.join(bundled_fonts_dir, "NotoSansCJK-Bold.ttc"),
            "noto_serif_cjk_bold": os.path.join(bundled_fonts_dir, "NotoSerifCJK-Bold.ttc"),
            "wqy_zenhei": os.path.join(bundled_fonts_dir, "wqy-zenhei.ttc"),
            "nimbus_sans_bold": os.path.join(bundled_fonts_dir, "NimbusSans-Bold.otf"),
            "noto_serif_bold": os.path.join(bundled_fonts_dir, "NotoSerif-Bold.ttf"),
            "arial": os.path.join(bundled_fonts_dir, "LiberationSans-Regular.ttf"),
            "helvetica": os.path.join(bundled_fonts_dir, "LiberationSans-Regular.ttf"),
            "times": os.path.join(bundled_fonts_dir, "LiberationSerif-Regular.ttf")
        }

        # æ„å»ºå®Œæ•´çš„å­—ä½“æ˜ å°„ï¼ˆå†…ç½®å­—ä½“ + ç³»ç»Ÿå­—ä½“ä½œä¸ºå¤‡é€‰ï¼‰
        import platform
        system = platform.system()

        font_mapping = {}

        # ä¸ºæ¯ä¸ªå­—ä½“æ·»åŠ å†…ç½®ç‰ˆæœ¬å’Œç³»ç»Ÿå¤‡é€‰ç‰ˆæœ¬
        for font_key in bundled_fonts:
            font_mapping[font_key] = [bundled_fonts[font_key]]

            # æ·»åŠ ç³»ç»Ÿå­—ä½“ä½œä¸ºå¤‡é€‰
            if system == "Linux":
                system_paths = {
                    "noto_cjk_regular": [
                        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
                        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc"
                    ],
                    "noto_cjk_bold": [
                        "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc",
                        "/usr/share/fonts/truetype/noto/NotoSansCJK-Bold.ttc"
                    ],
                    "wqy_zenhei": ["/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc"],
                    "arial": [
                        "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
                        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
                    ],
                    "helvetica": [
                        "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
                        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
                    ],
                    "times": [
                        "/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf",
                        "/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf"
                    ]
                }
            elif system == "Windows":
                system_paths = {
                    "noto_cjk_regular": ["C:/Windows/Fonts/NotoSansCJK-Regular.ttc"],
                    "noto_cjk_bold": ["C:/Windows/Fonts/NotoSansCJK-Bold.ttc"],
                    "wqy_zenhei": [],
                    "arial": ["C:/Windows/Fonts/arial.ttf"],
                    "helvetica": ["C:/Windows/Fonts/arial.ttf"],
                    "times": ["C:/Windows/Fonts/times.ttf"]
                }
            elif system == "Darwin":  # macOS
                system_paths = {
                    "noto_cjk_regular": ["/Library/Fonts/NotoSansCJK-Regular.ttc"],
                    "noto_cjk_bold": ["/Library/Fonts/NotoSansCJK-Bold.ttc"],
                    "wqy_zenhei": [],
                    "arial": ["/System/Library/Fonts/Arial.ttf"],
                    "helvetica": ["/System/Library/Fonts/Helvetica.ttc"],
                    "times": ["/System/Library/Fonts/Times.ttc"]
                }
            else:
                system_paths = {}

            # æ·»åŠ ç³»ç»Ÿè·¯å¾„ä½œä¸ºå¤‡é€‰
            if font_key in system_paths:
                font_mapping[font_key].extend(system_paths[font_key])

        # æ·»åŠ ä»…åœ¨æŸäº›å­—ä½“åŒ…ä¸­çš„ç‰¹æ®Šå­—ä½“
        if system == "Linux":
            font_mapping["noto_serif_cjk"] = [
                "/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc",
                "/usr/share/fonts/truetype/noto/NotoSerifCJK-Regular.ttc"
            ]
            font_mapping["droid_sans"] = [
                "/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf",
                "/usr/share/fonts/truetype/droid/DroidSans.ttf"
            ]

        return font_mapping

    def _get_bundled_fonts_dir(self):
        """è·å–å†…ç½®å­—ä½“åŒ…ç›®å½•è·¯å¾„"""
        # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        fonts_dir = os.path.join(current_dir, "fonts")

        if os.path.exists(fonts_dir):
            print(f"ğŸ”¤ æ‰¾åˆ°å†…ç½®å­—ä½“åŒ…: {fonts_dir}")
            return fonts_dir
        else:
            print(f"âš ï¸ å†…ç½®å­—ä½“åŒ…ä¸å­˜åœ¨: {fonts_dir}")
            return None

    def _find_best_font(self):
        """è‡ªåŠ¨æŸ¥æ‰¾æœ€ä½³å¯ç”¨å­—ä½“ï¼ˆä¼˜å…ˆå†…ç½®å­—ä½“ï¼‰"""
        # è·å–å†…ç½®å­—ä½“åŒ…è·¯å¾„
        bundled_fonts_dir = self._get_bundled_fonts_dir()

        # ä¼˜å…ˆçº§å­—ä½“åˆ—è¡¨ï¼ˆå†…ç½®å­—ä½“åŒ…ä¼˜å…ˆï¼‰
        priority_fonts = []

        if bundled_fonts_dir:
            # å†…ç½®å­—ä½“åŒ…å­—ä½“ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼Œç²—ä½“ä¼˜å…ˆç”¨äºæ ‡é¢˜ï¼‰
            bundled_priority = [
                os.path.join(bundled_fonts_dir, "NotoSansCJK-Bold.ttc"),     # Notoç²—ä½“ï¼ˆæ ‡é¢˜é¦–é€‰ï¼‰
                os.path.join(bundled_fonts_dir, "NotoSerifCJK-Bold.ttc"),    # Notoè¡¬çº¿ç²—ä½“
                os.path.join(bundled_fonts_dir, "wqy-zenhei.ttc"),           # æ–‡æ³‰é©¿æ­£é»‘
                os.path.join(bundled_fonts_dir, "NotoSansCJK-Regular.ttc"),  # Notoå¸¸è§„
                os.path.join(bundled_fonts_dir, "LiberationSans-Regular.ttf"), # Liberation Sans
            ]
            priority_fonts.extend(bundled_priority)

        # ç³»ç»Ÿå­—ä½“ä½œä¸ºå¤‡é€‰
        import platform
        system = platform.system()

        if system == "Linux":
            system_fonts = [
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",  # Notoä¸­æ—¥éŸ©å­—ä½“
                "/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc",  # æ–‡æ³‰é©¿æ­£é»‘
                "/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf",  # Droidå­—ä½“
                "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",  # æ–‡æ³‰é©¿å¾®ç±³é»‘
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc",  # Notoç²—ä½“
                "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",  # Liberation
            ]
        elif system == "Windows":
            system_fonts = [
                "C:/Windows/Fonts/msyh.ttc",    # å¾®è½¯é›…é»‘
                "C:/Windows/Fonts/simhei.ttf",  # é»‘ä½“
                "C:/Windows/Fonts/simsun.ttc",  # å®‹ä½“
                "C:/Windows/Fonts/simkai.ttf",  # æ¥·ä½“
                "C:/Windows/Fonts/arial.ttf",   # Arial
            ]
        elif system == "Darwin":  # macOS
            system_fonts = [
                "/System/Library/Fonts/PingFang.ttc",
                "/Library/Fonts/STHeiti Light.ttc",
                "/System/Library/Fonts/STHeiti Medium.ttc",
                "/Library/Fonts/Arial Unicode MS.ttf",
                "/System/Library/Fonts/Arial.ttf",
            ]
        else:
            system_fonts = []

        priority_fonts.extend(system_fonts)

        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå­˜åœ¨çš„å­—ä½“
        for font_path in priority_fonts:
            if os.path.exists(font_path):
                source = "å†…ç½®å­—ä½“åŒ…" if bundled_fonts_dir and font_path.startswith(bundled_fonts_dir) else "ç³»ç»Ÿå­—ä½“"
                print(f"ğŸ”¤ è‡ªåŠ¨é€‰æ‹©å­—ä½“: {font_path} ({source})")
                return font_path

        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨å­—ä½“")
        return None

    def _get_timestamp(self):
        """è·å–æ—¶é—´æˆ³"""
        import time
        return str(int(time.time()))


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "VideoComposer": VideoComposer
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoComposer": "ğŸ¬ è§†é¢‘åˆæˆå™¨"
}